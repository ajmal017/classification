{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TUNER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtiqwXZU4dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yahoofinancials\n",
        "!pip install pandas-ta\n",
        "!pip install catboost\n",
        "!pip install costcla\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEB_fBD1PBWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#IMPORT CLASSIFIERS\n",
        "#IMPORT CLASSIFIERS\n",
        "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from costcla.models import CostSensitiveRandomForestClassifier\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "#IMPORT OTHERS\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import scipy.stats as st\n",
        "import pandas_ta as ta\n",
        "import time\n",
        "import warnings\n",
        "import itertools\n",
        "import hyperopt as hp\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, plot_confusion_matrix, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from datetime import datetime\n",
        "from yahoofinancials import YahooFinancials\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split,  cross_val_score, KFold, cross_val_score\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.combine import SMOTETomek\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hinsS6qAfo_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS ONE FOR CONFUSION MATRIX\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names, \n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=False):\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bEUTB30YJ3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HELP():\n",
        "    def __init__( self ):\n",
        "        self.classifier_list = [ ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier,\n",
        "                                GradientBoostingClassifier, SVC, MLPClassifier, KNeighborsClassifier,\n",
        "                                CatBoostClassifier, LGBMClassifier,  XGBClassifier, StackingCVClassifier,\n",
        "                                CostSensitiveRandomForestClassifier ]\n",
        "\n",
        "        self.classifier_list_str = [ 'ExtraTreesClassifier','RandomForestClassifier', 'AdaBoostClassifier',\n",
        "                                'GradientBoostingClassifier', 'SVC', 'MLPClassifier', 'KNeighborsClassifier',\n",
        "                                'CatBoostClassifier', 'LGBMClassifier',  'XGBClassifier', 'StackingCVClassifier',\n",
        "                                'CostSensitiveRandomForestClassifier' ]\n",
        "\n",
        "        #self.class_list_spike = list()\n",
        "        #self.class_list_crash = list()\n",
        "        #self.class_key_spike = list()\n",
        "        #self.class_key_crash = list()\n",
        "        #self.last_x_days = last_x_days\n",
        "        #self.cycle = cycle\n",
        "        #self.counter = 0\n",
        "        #self.counter_max = last_x_days // cycle -1 if last_x_days % cycle == 0 else last_x_days // cycle \n",
        "        #self.last_cycle = self.cycle if last_x_days % cycle == 0 else last_x_days % cycle\n",
        "        #self.preprocess()\n",
        "        #self.define_classes( return_days, multicollinearity )\n",
        "        #self.DATE_TEST = self.DATE_ALL_DATA.iloc[ -last_x_days : ]\n",
        "        #self.TEST_RETURNS = self.ALL_RETURNS[ -last_x_days : ]\n",
        "\n",
        "    def preprocess( self ):\n",
        "        print('\\n PREPROCESSING \\n')\n",
        "\n",
        "        ticker_details = pd.read_excel('Ticker List.xlsx')\n",
        "        ticker = ticker_details['Ticker'].to_list()\n",
        "        names = ticker_details['Description'].to_list()\n",
        "\n",
        "        #Extracting Data from Yahoo Finance and Adding them to Values table using date as key\n",
        "        end_date= \"2020-06-19\"\n",
        "        start_date = \"2000-01-01\"\n",
        "        date_range = pd.bdate_range(start=start_date,end=end_date)\n",
        "        values = pd.DataFrame({ 'Date': date_range})\n",
        "        values['Date']= pd.to_datetime(values['Date'])\n",
        "\n",
        "        #Extracting Data from Yahoo Finance and Adding them to Values table using date as key\n",
        "        for i in ticker:\n",
        "            raw_data = YahooFinancials(i)\n",
        "            raw_data = raw_data.get_historical_price_data(start_date, end_date, \"daily\")\n",
        "            df = pd.DataFrame(raw_data[i]['prices'])[['formatted_date','adjclose']]\n",
        "            df.columns = ['Date1',i]\n",
        "            df['Date1']= pd.to_datetime(df['Date1'])\n",
        "            values = values.merge(df,how='left',left_on='Date',right_on='Date1')\n",
        "            values = values.drop(labels='Date1',axis=1)\n",
        "        self.VV = values\n",
        "\n",
        "        #Renaming columns to represent instrument names rather than their ticker codes for ease of readability\n",
        "        names.insert(0,'Date')\n",
        "        values.columns = names\n",
        "        values.tail()\n",
        "\n",
        "\n",
        "        #Front filling the NaN values in the data set\n",
        "        values = values.fillna(method=\"ffill\",axis=0)\n",
        "        values = values.fillna(method=\"bfill\",axis=0)\n",
        "        values.isna().sum()\n",
        "\n",
        "        #Return\n",
        "        values['SPX-RSI'] = ta.rsi( values['SPX'] )\n",
        "\n",
        "        BBANDS = ta.bbands( values['SPX'] )\n",
        "        keys = BBANDS.keys().to_list()\n",
        "\n",
        "        Upper = BBANDS[ 'BBU_5' ]\n",
        "        Lower = BBANDS[ 'BBL_5' ]\n",
        "\n",
        "        Upper_perc = Upper / values['SPX']\n",
        "        Lower_perc = Lower / values['SPX']\n",
        "\n",
        "        values[ 'BBU-Distance' ] = Upper_perc\n",
        "        values[ 'BBL-Distance' ] = Lower_perc\n",
        "        values['MACD-Histogram'] = ta.macd( values[ 'SPX' ] )[ 'MACDH_12_26_9' ]\n",
        "        # Co-ercing numeric type to all columns except Date\n",
        "        cols=values.columns.drop('Date')\n",
        "        values[cols] = values[cols].apply(pd.to_numeric,errors='coerce').round(decimals=4)\n",
        "        #print(values.tail())\n",
        "\n",
        "        imp = ['Gold','USD Index', 'Oil', 'SPX','VIX', 'High Yield Fund' , 'Nikkei', 'Dax', '10Yr', '2Yr' , 'EEM' ,'XLE', 'XLF', 'XLI', 'AUDJPY']\n",
        "        # Calculating Short term -Historical Returns\n",
        "        change_days = [1,3,5,14,21]\n",
        "        \n",
        "        data = pd.DataFrame(data=values['Date'])\n",
        "        for i in change_days:\n",
        "            x= values[cols].pct_change(periods=i).add_suffix(\"-T-\"+str(i))\n",
        "            data=pd.concat(objs=(data,x),axis=1)\n",
        "            x=[]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculating Long term Historical Returns\n",
        "        change_days = [60,90,180,250]\n",
        "\n",
        "        for i in change_days:\n",
        "            x= values[imp].pct_change(periods=i).add_suffix(\"-T-\"+str(i))\n",
        "            data=pd.concat(objs=(data,x),axis=1)\n",
        "            x=[]\n",
        "\n",
        "        #Calculating Moving averages for SPX\n",
        "        moving_avg = pd.DataFrame(values['Date'],columns=['Date'])\n",
        "        moving_avg['Date']=pd.to_datetime(moving_avg['Date'],format='%Y-%b-%d')\n",
        "        moving_avg['SPX/15SMA'] = (values['SPX']/(values['SPX'].rolling(window=15).mean()))-1\n",
        "        moving_avg['SPX/30SMA'] = (values['SPX']/(values['SPX'].rolling(window=30).mean()))-1\n",
        "        moving_avg['SPX/60SMA'] = (values['SPX']/(values['SPX'].rolling(window=60).mean()))-1\n",
        "        moving_avg['SPX/90SMA'] = (values['SPX']/(values['SPX'].rolling(window=90).mean()))-1\n",
        "        moving_avg['SPX/180SMA'] = (values['SPX']/(values['SPX'].rolling(window=180).mean()))-1\n",
        "        moving_avg['SPX/90EMA'] = (values['SPX']/(values['SPX'].ewm(span=90,adjust=True,ignore_na=True).mean()))-1\n",
        "        moving_avg['SPX/180EMA'] = (values['SPX']/(values['SPX'].ewm(span=180,adjust=True,ignore_na=True).mean()))-1\n",
        "        moving_avg = moving_avg.dropna(axis=0)\n",
        "        #Merging Moving Average values to the feature space\n",
        "        data['Date']=pd.to_datetime(data['Date'],format='%Y-%b-%d')\n",
        "\n",
        "\n",
        "        self.RAW_data = pd.merge(left=data,right=moving_avg,how='left',on='Date')\n",
        "\n",
        "        self.RAW_y = pd.DataFrame(data=values['Date'])\n",
        "\n",
        "        self.RAW_values = values\n",
        "\n",
        "    def mult( self, data, threshold, TARGET = 'SPX' ):\n",
        "        keys = data.keys().to_list()\n",
        "        corr = data.corr()\n",
        "        target_set = corr[ TARGET ]\n",
        "        drop_list = list()\n",
        "        \n",
        "        #loop over features in corr matrix except SPX\n",
        "        for i, key in enumerate(keys):\n",
        "            if key != TARGET:\n",
        "                \n",
        "                print('EXAMINING {} \\n '.format( key ) )\n",
        "                print('DROP_LIST {} \\n'.format( drop_list ) )\n",
        "                #Get 'abs' corr values of current feature \n",
        "                values = corr[ key ]\n",
        "                \n",
        "                #we wont control corr between feature and [ SPX, features itself ] \n",
        "                #we will also skip dropped features in the next loop\n",
        "                skip_list = [ TARGET, key ]\n",
        "\n",
        "                #since skip_list will be updated for each feature\n",
        "                #we save dropped features in drop_list to append it \n",
        "                #while exploring each feature\n",
        "                if len( drop_list ) > 0:\n",
        "                    #drop if drop_list is not empty\n",
        "                    skip_list.extend( drop_list )\n",
        "\n",
        "                print('SKIP LIST {} \\n'.format( skip_list ) )\n",
        "                #we will loop over \n",
        "                for k, ind in enumerate(keys):\n",
        "                    #skip features in skip_list\n",
        "                    if ind not in skip_list:\n",
        "\n",
        "                        print(ind)\n",
        "                        print(ind)\n",
        "                        #if corr between 2 features exceeds threshold\n",
        "                        if abs( values[ k ] ) > threshold:\n",
        "                            print( values[ k ] )\n",
        "                            #check which one is more corr to target\n",
        "                            if corr[ TARGET ][ key ] > corr[ TARGET ][ ind ]:\n",
        "                                #drop the other one\n",
        "                                data = data.drop( ind, axis = 1 )\n",
        "                                #append dropped feature's key to drop_list\n",
        "                                drop_list.append( ind )\n",
        "                                print('DROPPED {} \\n '.format( ind ))\n",
        "                            elif key not in drop_list:\n",
        "                                #inverse of the 'statement' above\n",
        "                                data = data.drop( key, axis = 1 )\n",
        "                                drop_list.append( key )\n",
        "                                print('DROPPED {} \\n '.format( key ))\n",
        "                                break\n",
        "\n",
        "        return data, drop_list\n",
        "                    \n",
        "            \n",
        "    def define_classes( self, return_days, multicollinearity ):\n",
        "        print('\\n DEFINING CLASSES \\n')\n",
        "        y = self.RAW_y\n",
        "        values = self.RAW_values\n",
        "        data = self.RAW_data\n",
        "        \n",
        "        y[ 'SPX-Target' ] = values[ \"SPX\" ].pct_change( periods = -return_days )\n",
        "        y.isna().sum()\n",
        "\n",
        "        # Removing NAs\n",
        "        data = data[ data[ 'SPX-T-250' ].notna() ]\n",
        "        y = y[ y[ 'SPX-Target' ].notna() ]\n",
        "\n",
        "        #Adding Target Variables\n",
        "        data = pd.merge(left=data,right=y,how='inner',on='Date',suffixes=(False,False))\n",
        "        data.isna().sum()\n",
        "        \n",
        "        #Select Threshold p (left tail probability)\n",
        "        p1 = 0.25\n",
        "        p2 = 0.75\n",
        "        #Get z-Value\n",
        "        z1 = st.norm.ppf( p1 )\n",
        "        z2 = st.norm.ppf( p2 )\n",
        "       \n",
        "        #Calculating Threshold (t) for each Y\n",
        "        crash = round( (z1 * np.std( data[ \"SPX-Target\" ] ) ) + np.mean( data[ \"SPX-Target\" ] ), 5 )\n",
        "        spike = round( (z2 * np.std( data[ \"SPX-Target\" ] ) ) + np.mean( data[ \"SPX-Target\" ] ), 5 )\n",
        "        \n",
        "        #Creating Labels\n",
        "        data[ 'Y-Crash' ] = ( data[ 'SPX-Target' ] < crash ) * 1\n",
        "        data[ 'Y-Spike' ] = ( data[ 'SPX-Target' ] > spike ) * 1\n",
        "\n",
        "        #Save Dates\n",
        "        self.DATE_ALL_DATA = data[ 'Date' ]\n",
        "        last_values = values[ values[ 'Date' ] <= data[ 'Date' ].iloc[ -1 ] ]\n",
        "        self.ALL_RETURNS = list()\n",
        "\n",
        "        #Calculate cash returns\n",
        "        RATES = data[ 'SPX-Target' ].to_list()\n",
        "        CLOSE = last_values[ 'SPX' ].to_list()\n",
        "\n",
        "        for i in range( len( data[ 'SPX-Target' ] ) ):\n",
        "            self.ALL_RETURNS.append( RATES[ i ] * CLOSE[ i ] )\n",
        "\n",
        "        #Prepare df for vif calculation\n",
        "        no_target_data =  data.drop( [ 'SPX-Target', 'Date', 'Y-Crash', 'Y-Spike'  ], axis = 1 )\n",
        "        #calculate vif\n",
        "        self.vif_scores = self.variance_inflation_factors( no_target_data )\n",
        "        #get feature names which vif > 10\n",
        "        inds = (self.vif_scores > 10 ).index\n",
        "        droplist = inds[ self.vif_scores > 10 ]\n",
        "\n",
        "        if multicollinearity == True:\n",
        "            data = data.drop( droplist, axis = 1 )\n",
        "\n",
        "        #For now, data has Spike and Crash labels \n",
        "        self.data = data.drop( [ 'SPX-Target', 'Date'], axis = 1 )\n",
        "        \n",
        "\n",
        "    def variance_inflation_factors( self, exog_df ):\n",
        "\n",
        "        exog_df = add_constant(exog_df)\n",
        "        vifs = pd.Series(\n",
        "            [1 / (1. - OLS(exog_df[col].values, \n",
        "                          exog_df.loc[:, exog_df.columns != col].values).fit().rsquared) \n",
        "            for col in exog_df],\n",
        "            index=exog_df.columns,\n",
        "            name='VIF'\n",
        "        )\n",
        "        return vifs\n",
        "\n",
        "\n",
        "    def select_classifier( self, classifier, key ):\n",
        "        #for ELEM in self.classifier_list_str:\n",
        "        #    if classifier == ELEM[ : len( classifier )  ]: \n",
        "        #        self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ) ]\n",
        "        if key == 'crash':\n",
        "            if classifier == 'EXT':\n",
        "                self.class_list_crash.append( ExtraTreesClassifier( n_estimators = 300, random_state = 0 ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "            elif classifier == 'GB':\n",
        "                self.class_list_crash.append( GradientBoostingClassifier( random_state = 0 ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "                    \n",
        "            elif classifier == 'CB':\n",
        "                self.class_list_crash.append( CatBoostClassifier() )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "            elif classifier == 'STCK':\n",
        "                est_list = list()\n",
        "                for i in range( len( self.class_list_crash ) ):\n",
        "                    est_list.append( ( self.class_key_crash[ i ], self.class_list_crash[ i ]  ) )\n",
        "                self.class_list_crash.append( StackingClassifier(estimators = est_list,  cv = 5, final_estimator = CatBoostClassifier() ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "        elif key == 'spike':\n",
        "            if classifier == 'EXT':\n",
        "                self.class_list_spike.append( ExtraTreesClassifier( n_estimators = 300, random_state = 0 ) )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'GB':\n",
        "                self.class_list_spike.append( GradientBoostingClassifier( random_state = 0 ) )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'CB':\n",
        "                self.class_list_spike.append( CatBoostClassifier() )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'STCK':\n",
        "                est_list = list()\n",
        "                for i in range( len(self.class_list_spike) ):\n",
        "                    est_list.append( ( self.class_key_spike[ i ], self.class_list_spike[ i ]  ) )\n",
        "                self.class_list_spike.append( StackingClassifier( estimators = est_list,  cv = 5, final_estimator = CatBoostClassifier() ) ) \n",
        "\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "    def backtest(self):\n",
        "        list_of_cash_tt = list()\n",
        "        self.make_pred( for_valid = False )\n",
        "        classifiers = self.BT_RESULTS.keys().to_list()\n",
        "        count = 0\n",
        "        for CCC in classifiers:\n",
        "            if CCC != 'Target':\n",
        "                temp_cash = list()\n",
        "                temp_cash.append( 0 )\n",
        "                cc = 0\n",
        "\n",
        "                preds = self.BT_RESULTS[ CCC ].to_list()\n",
        "\n",
        "                assert len( preds ) ==  len( self.TEST_RETURNS )\n",
        "\n",
        "                for i in range( len( preds ) ):\n",
        "                    if preds[ i ] == 1:\n",
        "                        cc += -self.TEST_RETURNS[ i ]\n",
        "                    temp_cash.append( cc )\n",
        "\n",
        "                if count == 0:\n",
        "                    CASH = pd.DataFrame( { CCC: temp_cash  })\n",
        "                    count += 1\n",
        "                else:\n",
        "                    CASH[ CCC ] = temp_cash\n",
        "                    count += 1\n",
        "\n",
        "        self.CASH = CASH\n",
        "      \n",
        "\n",
        "\n",
        "    def scores( self, typez ):\n",
        "        if typez == 'crash':\n",
        "            results = self.results_crash\n",
        "        else:\n",
        "            results = self.results_spike\n",
        "\n",
        "        KEYS = results.keys().to_list()\n",
        "        counter = 0\n",
        "        for  key in KEYS:\n",
        "            if key != 'Target':\n",
        "                score_list = list()\n",
        "                for metric in self.metrics:\n",
        "                    score_list.append( metric( results[ 'Target' ], results[ key ] ) )\n",
        "\n",
        "                if counter == 0:\n",
        "                    scores = pd.DataFrame( { key: score_list  })\n",
        "                    counter = counter + 1\n",
        "                else:\n",
        "                    scores[ key ] = score_list\n",
        "                    counter = counter + 1\n",
        "\n",
        "        if typez == 'crash':\n",
        "            self.scores_crash = scores\n",
        "        else:\n",
        "            self.scores_spike = scores\n",
        "\n",
        "\n",
        "\n",
        "    def redefine_classifiers( self ):\n",
        "        self.class_list_spike = list()\n",
        "        self.class_list_crash = list()\n",
        "        for classifier in self.class_key_spike:\n",
        "            self.select_classifier( classifier, 'spike' )\n",
        "        for classifier in self.class_key_crash:\n",
        "            self.select_classifier( classifier, 'crash' )\n",
        "\n",
        "\n",
        "    def retrain( self ):\n",
        "\n",
        "        for i in range( self.counter_max + 1 ):\n",
        "            start_time = time.time()\n",
        "            print( '\\n TRAIN # {} \\n'.format( i ) )\n",
        "            self.update_data()\n",
        "            self.redefine_classifiers()\n",
        "            for classifier in self.class_list_spike:\n",
        "                classifier.fit( self.X_train_spike, self.y_train_spike )\n",
        "\n",
        "            for classifier in self.class_list_crash:\n",
        "                classifier.fit( self.X_train_crash, self.y_train_crash )\n",
        "            print('FINISHED {} \\n '.format( i ) )\n",
        "            print('Lasts : {} \\n' .format(time.time() - start_time))\n",
        "\n",
        "            self.make_pred( typez = 'crash' )\n",
        "            self.make_pred( typez = 'spike' )\n",
        "            self.counter += 1\n",
        "\n",
        "\n",
        "    def smotetomek( self, X_train, y_train ):\n",
        "        smt = SMOTETomek( random_state = 21 )\n",
        "        return smt.fit_sample( X_train, y_train )\n",
        "\n",
        "\n",
        "    def update_data( self ):\n",
        "\n",
        "        #Update train-test split after every cycle time\n",
        "        #if it is last cycle, use remaining days as test data\n",
        "        if self.counter_max == self.counter:\n",
        "            TEST_data = self.data.iloc[ -self.last_x_days + self.counter * self.cycle : ]\n",
        "            TRAIN_data = self.data.iloc[ : -self.last_x_days + self.counter * self.cycle ]\n",
        "        else:\n",
        "            TEST_data = self.data.iloc[ -self.last_x_days + self.counter * self.cycle : -self.last_x_days + self.counter * self.cycle + self.cycle ]\n",
        "            TRAIN_data = self.data.iloc[ : -self.last_x_days + self.counter * self.cycle ]\n",
        "\n",
        "\n",
        "        #Create Training set and crash-spike labels\n",
        "        X_train = TRAIN_data.drop( ['Y-Crash', 'Y-Spike'], axis = 1 )\n",
        "        y_train_crash = TRAIN_data[ 'Y-Crash' ]\n",
        "        y_train_spike = TRAIN_data[ 'Y-Spike' ]\n",
        "\n",
        "        #Create test sets\n",
        "        self.X_test = TEST_data.drop( [ 'Y-Spike', 'Y-Crash' ], axis = 1 )\n",
        "        self.y_test_crash = TEST_data[ 'Y-Crash' ]\n",
        "        self.y_test_spike = TEST_data[ 'Y-Spike' ]\n",
        "\n",
        "        #Scale data between 0-1\n",
        "        mm = MinMaxScaler()\n",
        "        mm = mm.fit( X_train )\n",
        "        X_train = mm.transform( X_train )\n",
        "        self.X_test = mm.transform( self.X_test )\n",
        "\n",
        "        #Create samples with smotetomek to eliminate imbalance of positive instances\n",
        "        self.X_train_crash, self.y_train_crash = self.smotetomek( X_train, y_train_crash )\n",
        "        self.X_train_spike, self.y_train_spike = self.smotetomek( X_train, y_train_spike )\n",
        "\n",
        "\n",
        "    def make_pred( self, typez ):\n",
        "        XXX = self.X_test\n",
        "        if typez == 'crash':\n",
        "            TARGET = self.y_test_crash\n",
        "            classifiers = self.class_list_crash\n",
        "            class_keys = self.class_key_crash\n",
        "        else:\n",
        "            TARGET = self.y_test_spike\n",
        "            classifiers = self.class_list_spike\n",
        "            class_keys = self.class_key_spike\n",
        "\n",
        "        for i, classifier in enumerate( classifiers ):\n",
        "            temp = classifier.predict( XXX )\n",
        "            if i == 0:\n",
        "                results = pd.DataFrame( { 'Target': TARGET, class_keys[ i ] : temp } )\n",
        "            else:\n",
        "                results[ class_keys[ i ] ] = temp\n",
        "       \n",
        "\n",
        "        if typez == 'crash':\n",
        "            if self.counter == 0:\n",
        "                self.results_crash = results\n",
        "            else:\n",
        "                self.results_crash = self.results_crash.append( results )\n",
        "\n",
        "        else:\n",
        "            if self.counter == 0:\n",
        "                self.results_spike = results\n",
        "            else:\n",
        "\n",
        "                self.results_crash = self.results_crash.append( results )\n",
        "\n",
        "\n",
        "    def confusion_matrix( self, classifier, key ):\n",
        "        if key == 'crash':\n",
        "            pred = self.results_crash[ classifier ]\n",
        "            true = self.results_crash[ 'Target' ]\n",
        "            return confusion_matrix( true, pred )\n",
        "        elif key == 'spike':\n",
        "            pred = self.results_spike[ classifier ]\n",
        "            true = self.results_spike[ 'Target' ]\n",
        "            return confusion_matrix( true, pred )\n",
        "\n",
        "\n",
        "    def cash_test( self, preds ):\n",
        "        cash = [ 0 for i in range( 13 ) ]\n",
        "        CASHH = 0\n",
        "        cash_gt = [ 0 for i in range( 13 ) ]\n",
        "        CASHH_GT = 0\n",
        "        test_ret_values = list( self.TEST_RETURNS )\n",
        "        self.y_test = list(self.y_test)\n",
        "        for i in range( len( preds ) ):\n",
        "            CASHH = CASHH + test_ret_values[ i ] * preds[ i ]\n",
        "            cash.append( CASHH )\n",
        "            CASHH_GT = CASHH_GT + test_ret_values[ i ] * preds[ i ]\n",
        "            cash_gt.append( CASHH_GT )\n",
        "        return CASHH, CASHH_GT\n",
        "\n",
        "\n",
        "    def confusion_matrix( self ):\n",
        "        pred = self.optimizer( self.X_test )\n",
        "        true = self.y_test\n",
        "        cm = np.array(confusion_matrix( true, pred ))\n",
        "        plot_confusion_matrix( cm = cm, target_names = [ 'nothing', 'spike' ] )\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhfFqjOEdSPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tuner( HELP ):\n",
        "    def __init__( self, multicollinearity = False, typez = 'spike' ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.metrics = [ f1_score, precision_score, roc_auc_score ]\n",
        "        self.metircs_key = [ 'f1', 'precision', 'roc_auc']\n",
        "        self.classifier_list = [ ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier,\n",
        "                                GradientBoostingClassifier, SVC, MLPClassifier, KNeighborsClassifier,\n",
        "                                CatBoostClassifier, LGBMClassifier,  XGBClassifier, StackingCVClassifier,\n",
        "                                CostSensitiveRandomForestClassifier ]\n",
        "\n",
        "        self.classifier_list_str = [ 'ExtraTreesClassifier','RandomForestClassifier', 'AdaBoostClassifier',\n",
        "                                'GradientBoostingClassifier', 'SVC', 'MLPClassifier', 'KNeighborsClassifier',\n",
        "                                'CatBoostClassifier', 'LGBMClassifier',  'XGBClassifier', 'StackingCVClassifier',\n",
        "                                'CostSensitiveRandomForestClassifier' ]\n",
        "        self.typez = typez\n",
        "\n",
        "    def choose_classifier( self, classifier, backtest = False ):\n",
        "        self.class_key = classifier\n",
        "        for ELEM in self.classifier_list_str:\n",
        "            if classifier == ELEM[ : len( classifier )  ]: \n",
        "                if backtest:\n",
        "                    self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]( **self.best_params )\n",
        "                else:\n",
        "                    self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]()\n",
        "\n",
        "\n",
        "    def classifier_to_tune( self, classifier, backtest = False ):\n",
        "        self.class_key = classifier\n",
        "        for ELEM in self.classifier_list_str:\n",
        "            if classifier == ELEM[ : len( classifier )  ]: \n",
        "                self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]\n",
        "\n",
        "\n",
        "    def scale( self, X_train, X_test, scale_type = 'minmax' ):\n",
        "        if scale_type == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        elif scale_type == 'robust':\n",
        "            scaler = RobustScaler()\n",
        "        elif scale_type == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif scale_type == 'maxabs':\n",
        "            scaler = MaxAbsScaler()\n",
        "\n",
        "        scaler = scaler.fit( X_train )\n",
        "        X_train = scaler.transform( X_train )\n",
        "        X_test = scaler.transform( X_test )\n",
        "        \n",
        "        return X_train, X_test\n",
        "\n",
        "\n",
        "    def prepare_data( self, smotetomek = True, b_test = False, test_percent = 0.3 ):\n",
        "        shuffle = True\n",
        "        data = self.data\n",
        "            \n",
        "        X = data.drop( ['Y-Crash', 'Y-Spike'], axis = 1 )\n",
        "\n",
        "        if self.typez == 'crash':\n",
        "            y = data[ 'Y-Crash' ]\n",
        "        else:\n",
        "            y = data[ 'Y-Spike' ]\n",
        "\n",
        "        stratify = y\n",
        "\n",
        "        if b_test:\n",
        "            shuffle = False\n",
        "            stratify = None\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = test_percent, random_state = 42, shuffle = shuffle, stratify = stratify )\n",
        "\n",
        "        if smotetomek:\n",
        "            X_train, y_train = self.smotetomek( X_train, y_train )\n",
        "\n",
        "        X_train, X_test = self.scale( X_train, X_test )\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "    def process_data(self, return_days = 14, multicollinearity = False,\n",
        "                     smotetomek = True, b_test = False, test_percent = 0.2 ):\n",
        "      \n",
        "        self.preprocess()\n",
        "        self.define_classes( return_days, multicollinearity )\n",
        "        \n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = self.prepare_data( smotetomek = smotetomek,\n",
        "                                                                                  b_test = b_test,\n",
        "                                                                                  test_percent = test_percent)\n",
        "        \n",
        "    def print_class_options( self ):\n",
        "        for elem in self.classifier_list_str:\n",
        "            print('{} \\n'.format( elem ) )\n",
        "\n",
        "\n",
        "    def get_best_params( self ):\n",
        "        self.best_params = self.optimizer.best_params_\n",
        "        print( self.best_params )\n",
        "\n",
        "\n",
        "    def train_tuned( self ):\n",
        "        self.get_best_params()\n",
        "        self.process_data( b_test = True, test_size = 0.1 )\n",
        "        self.choose_classifier( self.class_key, backtest = True )\n",
        "        self.classifier.fit( self.X_train, self.y_train )\n",
        "        preds = self.classifier.predict( self.X_test )\n",
        "        cash, cash_gt = self.cash_test( preds )\n",
        "        return cash, cash_gt\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzj4nDfLWWld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HPOpt(Tuner):\n",
        "\n",
        "    def __init__(self, rosemary ):\n",
        "        self.X_train = rosemary.X_train\n",
        "        self.X_test  = rosemary.X_test\n",
        "        self.y_train = rosemary.y_train\n",
        "        self.y_test  = rosemary.y_test\n",
        "        self.classifier = rosemary.classifier\n",
        "\n",
        "    def process(self, fn_name, space, trials, algo, max_evals):\n",
        "        \n",
        "        fn = getattr(self, fn_name)\n",
        "        result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
        "\n",
        "        return result, trials\n",
        "\n",
        "\n",
        "    def objective( self, args ):\n",
        "        clf = self.classifier( **args )\n",
        "        clf.fit( self.X_train, self.y_train )\n",
        "        pred = clf.predict( self.X_test )\n",
        "        score = f1_score( self.y_test, pred )\n",
        "        return {'loss': -score, 'status': STATUS_OK}\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voDIK0bsWcai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHDcsMGHWcVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LnjUmk2PUmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rosemary = Tuner()\n",
        "rosemary.process_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEhw_OvvgNbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rosemary.classifier_to_tune( 'Cat' )"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0HrKFXY9Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqaRud9aZDyZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hhh = HPOpt( rosemary )\n",
        "asd = hhh.process( fn_name = 'objective', space = SPACE, trials = Trials(), algo = tpe.suggest, max_evals = 100 )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEKFDVi5jN0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "3ae4c92d-f28b-4a4b-81d9-cb1dac0bb949"
      },
      "source": [
        "asd"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'border_count': 79,\n",
              "  'depth': 6,\n",
              "  'feature_fraction': 1,\n",
              "  'iterations': 4,\n",
              "  'learning_rate': 0.08785192788447053},\n",
              " <hyperopt.base.Trials at 0x7fc2fc839780>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXaW0tptVxoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Displays available classifiers\n",
        "rosemary.print_class_options()\n",
        "#You can choose classifier by specifying first few letter\n",
        "rosemary.choose_classifier( 'Cat' )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6uZMztgKAk1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPACE = OrderedDict([('iterations', hp.choice('iterations', range(10, 15) )  ),\n",
        "                    ('learning_rate', hp.uniform('learning_rate', 0.001, 1 ) ),\n",
        "                    ('depth', hp.choice('depth', range(4, 12) ) ),\n",
        "                    ('border_count', hp.choice('border_count', range( 1, 200 ) ) ),\n",
        "                    ('grow_policy', hp.choice('feature_fraction', [ 'SymmetricTree', 'Depthwise', 'Lossguide' ] ) ) ] )\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5c964MEYBZE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rosemary.tune_classifier( tuning_args, num_of_iter = 30 )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43GU48DTYBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Display best params found\n",
        "rosemary.get_best_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xhlyQT3bahU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get test results\n",
        "test_pred = rosemary.optimizer.predict( rosemary.X_test )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK7oXTZepUTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate f1 score\n",
        "f1_score( rosemary.y_test, test_pred )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJYGU8EzY9tJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot confusion matrix\n",
        "rosemary.confusion_matrix()\n",
        "\n",
        "#if it does not work, try the code below. \n",
        "#You need to have test_pred first\n",
        "#cm = np.array(confusion_matrix( rosemary.y_test, test_pred ))\n",
        "#plot_confusion_matrix( cm = cm, target_names = [ 'nothing', 'spike' ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giHasdJMY9rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get back-test results\n",
        "pred_returns, returns_with_grand_truth = rosemary.train_tuned()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukxnTC4OaMHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot( pred_returns )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMuWJEOEY9pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eclLQCu4rvOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W_bOrWYsPK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u0H-f3krlv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cCaMnyGXdds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJXDgjw1Y9no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjyxXMp_Y9mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global X\n",
        "global Y\n",
        "\n",
        "X = rosemary.X_train\n",
        "Y = rosemary.y_train\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfdIR7oPLbfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "07dd6975-209b-458e-dc66-068717c48d62"
      },
      "source": [
        "sample( SPACE )"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'border_count': 112.08976256884324,\n",
              " 'depth': 5,\n",
              " 'grow_policy': 'Lossguide',\n",
              " 'iterations': 996.7360741510896,\n",
              " 'learning_rate': 0.018859030663855587}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LTFCWGeCWZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XWVswXiCc3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5q7d3bjSOVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1fb0a821-2df3-4ef4-b481-c0ab9dca7f07"
      },
      "source": [
        "print(best)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'border_count': 121.62068796436915, 'depth': 6, 'feature_fraction': 1, 'iterations': 14.456372424780248, 'learning_rate': 0.5409026700346178}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ODX4RZCfNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMtzD9BfRXhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trials.trials.results[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}