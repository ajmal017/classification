{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtiqwXZU4dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yahoofinancials\n",
        "!pip install pandas-ta\n",
        "!pip install catboost\n",
        "!pip install costcla\n",
        "!pip install scikit-optimize\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEB_fBD1PBWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "121fd4f6-bae1-4c13-f001-f2507105cf4c"
      },
      "source": [
        "#IMPORT CLASSIFIERS\n",
        "#IMPORT CLASSIFIERS\n",
        "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from costcla.models import CostSensitiveRandomForestClassifier\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "#IMPORT OTHERS\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import scipy.stats as st\n",
        "import pandas_ta as ta\n",
        "import time\n",
        "import warnings\n",
        "import itertools\n",
        "\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real, Categorical, Integer\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, plot_confusion_matrix, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from datetime import datetime\n",
        "from yahoofinancials import YahooFinancials\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split,  cross_val_score, KFold, cross_val_score\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.combine import SMOTETomek\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from inspect import signature, isclass, Parameter\n",
        "from functools import wraps, partial\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neural_network.multilayer_perceptron module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neural_network. Anything that cannot be imported from sklearn.neural_network is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRx5bx-WUTKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# THESE ARE JUST TO CALCULATE VAL SCORE ON TUNING ALGORITHM\n",
        "# WE MANIPULATED SOME SOURCE CODE\n",
        "def _cached_call(cache, estimator, method, *args, **kwargs):\n",
        "    \"\"\"Call estimator with method and args and kwargs.\"\"\"\n",
        "    if cache is None:\n",
        "        return getattr(estimator, method)(*args, **kwargs)\n",
        "\n",
        "    try:\n",
        "        return cache[method]\n",
        "    except KeyError:\n",
        "        result = getattr(estimator, method)(*args, **kwargs)\n",
        "        cache[method] = result\n",
        "        return result\n",
        "\n",
        "class _BaseScorer:\n",
        "    def __init__(self, score_func, sign, kwargs):\n",
        "        super().__init__()\n",
        "        self._kwargs = kwargs\n",
        "        self._score_func = score_func\n",
        "        self._sign = sign\n",
        "        # XXX After removing the deprecated scorers (v0.24) remove the\n",
        "        # XXX deprecation_msg property again and remove __call__'s body again\n",
        "        self._deprecation_msg = None\n",
        "\n",
        "    def __repr__(self):\n",
        "        kwargs_string = \"\".join([\", %s=%s\" % (str(k), str(v))\n",
        "                                 for k, v in self._kwargs.items()])\n",
        "        return (\"make_scorer(%s%s%s%s)\"\n",
        "                % (self._score_func.__name__,\n",
        "                   \"\" if self._sign > 0 else \", greater_is_better=False\",\n",
        "                   self._factory_args(), kwargs_string))\n",
        "\n",
        "    def __call__(self, estimator, X, y_true, sample_weight=None):\n",
        "     \n",
        "        if self._deprecation_msg is not None:\n",
        "            warnings.warn(self._deprecation_msg,\n",
        "                          category=FutureWarning,\n",
        "                          stacklevel=2)\n",
        "        scorez = self._score(partial(_cached_call, None), estimator, X, y_true,\n",
        "                                  sample_weight=sample_weight)\n",
        "        print(scorez)\n",
        "        return scorez\n",
        "\n",
        "    def _factory_args(self):\n",
        "        \"\"\"Return non-default make_scorer arguments for repr.\"\"\"\n",
        "        return \"\"\n",
        "\n",
        "\n",
        "class _PredictScorer(_BaseScorer):\n",
        "    def _score(self, method_caller, estimator, X, y_true, sample_weight=None):\n",
        "\n",
        "        y_pred = method_caller(estimator, \"predict\", X_VAL)\n",
        "        if sample_weight is not None:\n",
        "            return self._sign * self._score_func(Y_VAL, y_pred,\n",
        "                                                 sample_weight=sample_weight,\n",
        "                                                 **self._kwargs)\n",
        "        else:\n",
        "            return self._sign * self._score_func(Y_VAL, y_pred,\n",
        "                                                 **self._kwargs)\n",
        "\n",
        "\n",
        "\n",
        "def make_scorer(score_func, *, greater_is_better=True, needs_proba=False,\n",
        "                needs_threshold=False, **kwargs):\n",
        "    \n",
        "    sign = 1 if greater_is_better else -1\n",
        "    if needs_proba and needs_threshold:\n",
        "        raise ValueError(\"Set either needs_proba or needs_threshold to True,\"\n",
        "                         \" but not both.\")\n",
        "    if needs_proba:\n",
        "        cls = _ProbaScorer\n",
        "    elif needs_threshold:\n",
        "        cls = _ThresholdScorer\n",
        "    else:\n",
        "        cls = _PredictScorer\n",
        "    return cls(score_func, sign, kwargs)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hinsS6qAfo_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS ONE FOR CONFUSION MATRIX\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names, \n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=False):\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bEUTB30YJ3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HELP():\n",
        "    def __init__( self ):\n",
        "        self.classifier_list = [ ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier,\n",
        "                                GradientBoostingClassifier, SVC, MLPClassifier, KNeighborsClassifier,\n",
        "                                CatBoostClassifier, LGBMClassifier,  XGBClassifier, StackingCVClassifier,\n",
        "                                CostSensitiveRandomForestClassifier ]\n",
        "\n",
        "        self.classifier_list_str = [ 'ExtraTreesClassifier','RandomForestClassifier', 'AdaBoostClassifier',\n",
        "                                'GradientBoostingClassifier', 'SVC', 'MLPClassifier', 'KNeighborsClassifier',\n",
        "                                'CatBoostClassifier', 'LGBMClassifier',  'XGBClassifier', 'StackingCVClassifier',\n",
        "                                'CostSensitiveRandomForestClassifier' ]\n",
        "\n",
        "        #self.class_list_spike = list()\n",
        "        #self.class_list_crash = list()\n",
        "        #self.class_key_spike = list()\n",
        "        #self.class_key_crash = list()\n",
        "        #self.last_x_days = last_x_days\n",
        "        #self.cycle = cycle\n",
        "        #self.counter = 0\n",
        "        #self.counter_max = last_x_days // cycle -1 if last_x_days % cycle == 0 else last_x_days // cycle \n",
        "        #self.last_cycle = self.cycle if last_x_days % cycle == 0 else last_x_days % cycle\n",
        "        #self.preprocess()\n",
        "        #self.define_classes( return_days, multicollinearity )\n",
        "        #self.DATE_TEST = self.DATE_ALL_DATA.iloc[ -last_x_days : ]\n",
        "        #self.TEST_RETURNS = self.ALL_RETURNS[ -last_x_days : ]\n",
        "\n",
        "    def preprocess( self ):\n",
        "        print('\\n PREPROCESSING \\n')\n",
        "\n",
        "        ticker_details = pd.read_excel('Ticker List.xlsx')\n",
        "        ticker = ticker_details['Ticker'].to_list()\n",
        "        names = ticker_details['Description'].to_list()\n",
        "\n",
        "        #Extracting Data from Yahoo Finance and Adding them to Values table using date as key\n",
        "        end_date= \"2020-06-19\"\n",
        "        start_date = \"2000-01-01\"\n",
        "        date_range = pd.bdate_range(start=start_date,end=end_date)\n",
        "        values = pd.DataFrame({ 'Date': date_range})\n",
        "        values['Date']= pd.to_datetime(values['Date'])\n",
        "\n",
        "        #Extracting Data from Yahoo Finance and Adding them to Values table using date as key\n",
        "        for i in ticker:\n",
        "            raw_data = YahooFinancials(i)\n",
        "            raw_data = raw_data.get_historical_price_data(start_date, end_date, \"daily\")\n",
        "            df = pd.DataFrame(raw_data[i]['prices'])[['formatted_date','adjclose']]\n",
        "            df.columns = ['Date1',i]\n",
        "            df['Date1']= pd.to_datetime(df['Date1'])\n",
        "            values = values.merge(df,how='left',left_on='Date',right_on='Date1')\n",
        "            values = values.drop(labels='Date1',axis=1)\n",
        "        self.VV = values\n",
        "\n",
        "        #Renaming columns to represent instrument names rather than their ticker codes for ease of readability\n",
        "        names.insert(0,'Date')\n",
        "        values.columns = names\n",
        "        values.tail()\n",
        "\n",
        "\n",
        "        #Front filling the NaN values in the data set\n",
        "        values = values.fillna(method=\"ffill\",axis=0)\n",
        "        values = values.fillna(method=\"bfill\",axis=0)\n",
        "        values.isna().sum()\n",
        "\n",
        "        #Return\n",
        "        values['SPX-RSI'] = ta.rsi( values['SPX'] )\n",
        "\n",
        "        BBANDS = ta.bbands( values['SPX'] )\n",
        "        keys = BBANDS.keys().to_list()\n",
        "\n",
        "        Upper = BBANDS[ 'BBU_5' ]\n",
        "        Lower = BBANDS[ 'BBL_5' ]\n",
        "\n",
        "        Upper_perc = Upper / values['SPX']\n",
        "        Lower_perc = Lower / values['SPX']\n",
        "\n",
        "        values[ 'BBU-Distance' ] = Upper_perc\n",
        "        values[ 'BBL-Distance' ] = Lower_perc\n",
        "        values['MACD-Histogram'] = ta.macd( values[ 'SPX' ] )[ 'MACDH_12_26_9' ]\n",
        "        # Co-ercing numeric type to all columns except Date\n",
        "        cols=values.columns.drop('Date')\n",
        "        values[cols] = values[cols].apply(pd.to_numeric,errors='coerce').round(decimals=4)\n",
        "        #print(values.tail())\n",
        "\n",
        "        imp = ['Gold','USD Index', 'Oil', 'SPX','VIX', 'High Yield Fund' , 'Nikkei', 'Dax', '10Yr', '2Yr' , 'EEM' ,'XLE', 'XLF', 'XLI', 'AUDJPY']\n",
        "        # Calculating Short term -Historical Returns\n",
        "        change_days = [1,3,5,14,21]\n",
        "        \n",
        "        data = pd.DataFrame(data=values['Date'])\n",
        "        for i in change_days:\n",
        "            x= values[cols].pct_change(periods=i).add_suffix(\"-T-\"+str(i))\n",
        "            data=pd.concat(objs=(data,x),axis=1)\n",
        "            x=[]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculating Long term Historical Returns\n",
        "        change_days = [60,90,180,250]\n",
        "\n",
        "        for i in change_days:\n",
        "            x= values[imp].pct_change(periods=i).add_suffix(\"-T-\"+str(i))\n",
        "            data=pd.concat(objs=(data,x),axis=1)\n",
        "            x=[]\n",
        "\n",
        "        #Calculating Moving averages for SPX\n",
        "        moving_avg = pd.DataFrame(values['Date'],columns=['Date'])\n",
        "        moving_avg['Date']=pd.to_datetime(moving_avg['Date'],format='%Y-%b-%d')\n",
        "        moving_avg['SPX/15SMA'] = (values['SPX']/(values['SPX'].rolling(window=15).mean()))-1\n",
        "        moving_avg['SPX/30SMA'] = (values['SPX']/(values['SPX'].rolling(window=30).mean()))-1\n",
        "        moving_avg['SPX/60SMA'] = (values['SPX']/(values['SPX'].rolling(window=60).mean()))-1\n",
        "        moving_avg['SPX/90SMA'] = (values['SPX']/(values['SPX'].rolling(window=90).mean()))-1\n",
        "        moving_avg['SPX/180SMA'] = (values['SPX']/(values['SPX'].rolling(window=180).mean()))-1\n",
        "        moving_avg['SPX/90EMA'] = (values['SPX']/(values['SPX'].ewm(span=90,adjust=True,ignore_na=True).mean()))-1\n",
        "        moving_avg['SPX/180EMA'] = (values['SPX']/(values['SPX'].ewm(span=180,adjust=True,ignore_na=True).mean()))-1\n",
        "        moving_avg = moving_avg.dropna(axis=0)\n",
        "        #Merging Moving Average values to the feature space\n",
        "        data['Date']=pd.to_datetime(data['Date'],format='%Y-%b-%d')\n",
        "\n",
        "\n",
        "        self.RAW_data = pd.merge(left=data,right=moving_avg,how='left',on='Date')\n",
        "\n",
        "        self.RAW_y = pd.DataFrame(data=values['Date'])\n",
        "\n",
        "        self.RAW_values = values\n",
        "\n",
        "    def mult( self, data, threshold, TARGET = 'SPX' ):\n",
        "        keys = data.keys().to_list()\n",
        "        corr = data.corr()\n",
        "        target_set = corr[ TARGET ]\n",
        "        drop_list = list()\n",
        "        \n",
        "        #loop over features in corr matrix except SPX\n",
        "        for i, key in enumerate(keys):\n",
        "            if key != TARGET:\n",
        "                \n",
        "                print('EXAMINING {} \\n '.format( key ) )\n",
        "                print('DROP_LIST {} \\n'.format( drop_list ) )\n",
        "                #Get 'abs' corr values of current feature \n",
        "                values = corr[ key ]\n",
        "                \n",
        "                #we wont control corr between feature and [ SPX, features itself ] \n",
        "                #we will also skip dropped features in the next loop\n",
        "                skip_list = [ TARGET, key ]\n",
        "\n",
        "                #since skip_list will be updated for each feature\n",
        "                #we save dropped features in drop_list to append it \n",
        "                #while exploring each feature\n",
        "                if len( drop_list ) > 0:\n",
        "                    #drop if drop_list is not empty\n",
        "                    skip_list.extend( drop_list )\n",
        "\n",
        "                print('SKIP LIST {} \\n'.format( skip_list ) )\n",
        "                #we will loop over \n",
        "                for k, ind in enumerate(keys):\n",
        "                    #skip features in skip_list\n",
        "                    if ind not in skip_list:\n",
        "\n",
        "                        print(ind)\n",
        "                        print(ind)\n",
        "                        #if corr between 2 features exceeds threshold\n",
        "                        if abs( values[ k ] ) > threshold:\n",
        "                            print( values[ k ] )\n",
        "                            #check which one is more corr to target\n",
        "                            if corr[ TARGET ][ key ] > corr[ TARGET ][ ind ]:\n",
        "                                #drop the other one\n",
        "                                data = data.drop( ind, axis = 1 )\n",
        "                                #append dropped feature's key to drop_list\n",
        "                                drop_list.append( ind )\n",
        "                                print('DROPPED {} \\n '.format( ind ))\n",
        "                            elif key not in drop_list:\n",
        "                                #inverse of the 'statement' above\n",
        "                                data = data.drop( key, axis = 1 )\n",
        "                                drop_list.append( key )\n",
        "                                print('DROPPED {} \\n '.format( key ))\n",
        "                                break\n",
        "\n",
        "        return data, drop_list\n",
        "                    \n",
        "            \n",
        "    def define_classes( self, return_days, multicollinearity ):\n",
        "        print('\\n DEFINING CLASSES \\n')\n",
        "        y = self.RAW_y\n",
        "        values = self.RAW_values\n",
        "        data = self.RAW_data\n",
        "        \n",
        "        y[ 'SPX-Target' ] = values[ \"SPX\" ].pct_change( periods = -return_days )\n",
        "        y.isna().sum()\n",
        "\n",
        "        # Removing NAs\n",
        "        data = data[ data[ 'SPX-T-250' ].notna() ]\n",
        "        y = y[ y[ 'SPX-Target' ].notna() ]\n",
        "\n",
        "        #Adding Target Variables\n",
        "        data = pd.merge(left=data,right=y,how='inner',on='Date',suffixes=(False,False))\n",
        "        data.isna().sum()\n",
        "        \n",
        "        #Select Threshold p (left tail probability)\n",
        "        p1 = 0.25\n",
        "        p2 = 0.75\n",
        "        #Get z-Value\n",
        "        z1 = st.norm.ppf( p1 )\n",
        "        z2 = st.norm.ppf( p2 )\n",
        "       \n",
        "        #Calculating Threshold (t) for each Y\n",
        "        crash = round( (z1 * np.std( data[ \"SPX-Target\" ] ) ) + np.mean( data[ \"SPX-Target\" ] ), 5 )\n",
        "        spike = round( (z2 * np.std( data[ \"SPX-Target\" ] ) ) + np.mean( data[ \"SPX-Target\" ] ), 5 )\n",
        "        \n",
        "        #Creating Labels\n",
        "        data[ 'Y-Crash' ] = ( data[ 'SPX-Target' ] < crash ) * 1\n",
        "        data[ 'Y-Spike' ] = ( data[ 'SPX-Target' ] > spike ) * 1\n",
        "\n",
        "        #Save Dates\n",
        "        self.DATE_ALL_DATA = data[ 'Date' ]\n",
        "        last_values = values[ values[ 'Date' ] <= data[ 'Date' ].iloc[ -1 ] ]\n",
        "        self.ALL_RETURNS = list()\n",
        "\n",
        "        #Calculate cash returns\n",
        "        RATES = data[ 'SPX-Target' ].to_list()\n",
        "        CLOSE = last_values[ 'SPX' ].to_list()\n",
        "\n",
        "        for i in range( len( data[ 'SPX-Target' ] ) ):\n",
        "            self.ALL_RETURNS.append( RATES[ i ] * CLOSE[ i ] )\n",
        "\n",
        "        #Prepare df for vif calculation\n",
        "        no_target_data =  data.drop( [ 'SPX-Target', 'Date', 'Y-Crash', 'Y-Spike'  ], axis = 1 )\n",
        "        #calculate vif\n",
        "        self.vif_scores = self.variance_inflation_factors( no_target_data )\n",
        "        #get feature names which vif > 10\n",
        "        inds = (self.vif_scores > 10 ).index\n",
        "        droplist = inds[ self.vif_scores > 10 ]\n",
        "\n",
        "        if multicollinearity == True:\n",
        "            data = data.drop( droplist, axis = 1 )\n",
        "\n",
        "        #For now, data has Spike and Crash labels \n",
        "        self.data = data.drop( [ 'SPX-Target', 'Date'], axis = 1 )\n",
        "        \n",
        "\n",
        "    def variance_inflation_factors( self, exog_df ):\n",
        "\n",
        "        exog_df = add_constant(exog_df)\n",
        "        vifs = pd.Series(\n",
        "            [1 / (1. - OLS(exog_df[col].values, \n",
        "                          exog_df.loc[:, exog_df.columns != col].values).fit().rsquared) \n",
        "            for col in exog_df],\n",
        "            index=exog_df.columns,\n",
        "            name='VIF'\n",
        "        )\n",
        "        return vifs\n",
        "\n",
        "\n",
        "    def select_classifier( self, classifier, key ):\n",
        "        #for ELEM in self.classifier_list_str:\n",
        "        #    if classifier == ELEM[ : len( classifier )  ]: \n",
        "        #        self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ) ]\n",
        "        if key == 'crash':\n",
        "            if classifier == 'EXT':\n",
        "                self.class_list_crash.append( ExtraTreesClassifier( n_estimators = 300, random_state = 0 ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "            elif classifier == 'GB':\n",
        "                self.class_list_crash.append( GradientBoostingClassifier( random_state = 0 ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "                    \n",
        "            elif classifier == 'CB':\n",
        "                self.class_list_crash.append( CatBoostClassifier() )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "            elif classifier == 'STCK':\n",
        "                est_list = list()\n",
        "                for i in range( len( self.class_list_crash ) ):\n",
        "                    est_list.append( ( self.class_key_crash[ i ], self.class_list_crash[ i ]  ) )\n",
        "                self.class_list_crash.append( StackingClassifier(estimators = est_list,  cv = 5, final_estimator = CatBoostClassifier() ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "        elif key == 'spike':\n",
        "            if classifier == 'EXT':\n",
        "                self.class_list_spike.append( ExtraTreesClassifier( n_estimators = 300, random_state = 0 ) )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'GB':\n",
        "                self.class_list_spike.append( GradientBoostingClassifier( random_state = 0 ) )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'CB':\n",
        "                self.class_list_spike.append( CatBoostClassifier() )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'STCK':\n",
        "                est_list = list()\n",
        "                for i in range( len(self.class_list_spike) ):\n",
        "                    est_list.append( ( self.class_key_spike[ i ], self.class_list_spike[ i ]  ) )\n",
        "                self.class_list_spike.append( StackingClassifier( estimators = est_list,  cv = 5, final_estimator = CatBoostClassifier() ) ) \n",
        "\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "    def backtest(self):\n",
        "        list_of_cash_tt = list()\n",
        "        self.make_pred( for_valid = False )\n",
        "        classifiers = self.BT_RESULTS.keys().to_list()\n",
        "        count = 0\n",
        "        for CCC in classifiers:\n",
        "            if CCC != 'Target':\n",
        "                temp_cash = list()\n",
        "                temp_cash.append( 0 )\n",
        "                cc = 0\n",
        "\n",
        "                preds = self.BT_RESULTS[ CCC ].to_list()\n",
        "\n",
        "                assert len( preds ) ==  len( self.TEST_RETURNS )\n",
        "\n",
        "                for i in range( len( preds ) ):\n",
        "                    if preds[ i ] == 1:\n",
        "                        cc += -self.TEST_RETURNS[ i ]\n",
        "                    temp_cash.append( cc )\n",
        "\n",
        "                if count == 0:\n",
        "                    CASH = pd.DataFrame( { CCC: temp_cash  })\n",
        "                    count += 1\n",
        "                else:\n",
        "                    CASH[ CCC ] = temp_cash\n",
        "                    count += 1\n",
        "\n",
        "        self.CASH = CASH\n",
        "      \n",
        "\n",
        "\n",
        "    def scores( self, typez ):\n",
        "        if typez == 'crash':\n",
        "            results = self.results_crash\n",
        "        else:\n",
        "            results = self.results_spike\n",
        "\n",
        "        KEYS = results.keys().to_list()\n",
        "        counter = 0\n",
        "        for  key in KEYS:\n",
        "            if key != 'Target':\n",
        "                score_list = list()\n",
        "                for metric in self.metrics:\n",
        "                    score_list.append( metric( results[ 'Target' ], results[ key ] ) )\n",
        "\n",
        "                if counter == 0:\n",
        "                    scores = pd.DataFrame( { key: score_list  })\n",
        "                    counter = counter + 1\n",
        "                else:\n",
        "                    scores[ key ] = score_list\n",
        "                    counter = counter + 1\n",
        "\n",
        "        if typez == 'crash':\n",
        "            self.scores_crash = scores\n",
        "        else:\n",
        "            self.scores_spike = scores\n",
        "\n",
        "\n",
        "\n",
        "    def redefine_classifiers( self ):\n",
        "        self.class_list_spike = list()\n",
        "        self.class_list_crash = list()\n",
        "        for classifier in self.class_key_spike:\n",
        "            self.select_classifier( classifier, 'spike' )\n",
        "        for classifier in self.class_key_crash:\n",
        "            self.select_classifier( classifier, 'crash' )\n",
        "\n",
        "\n",
        "    def retrain( self ):\n",
        "\n",
        "        for i in range( self.counter_max + 1 ):\n",
        "            start_time = time.time()\n",
        "            print( '\\n TRAIN # {} \\n'.format( i ) )\n",
        "            self.update_data()\n",
        "            self.redefine_classifiers()\n",
        "            for classifier in self.class_list_spike:\n",
        "                classifier.fit( self.X_train_spike, self.y_train_spike )\n",
        "\n",
        "            for classifier in self.class_list_crash:\n",
        "                classifier.fit( self.X_train_crash, self.y_train_crash )\n",
        "            print('FINISHED {} \\n '.format( i ) )\n",
        "            print('Lasts : {} \\n' .format(time.time() - start_time))\n",
        "\n",
        "            self.make_pred( typez = 'crash' )\n",
        "            self.make_pred( typez = 'spike' )\n",
        "            self.counter += 1\n",
        "\n",
        "\n",
        "    def smotetomek( self, X_train, y_train ):\n",
        "        smt = SMOTETomek( random_state = 21 )\n",
        "        return smt.fit_sample( X_train, y_train )\n",
        "\n",
        "\n",
        "    def update_data( self ):\n",
        "\n",
        "        #Update train-test split after every cycle time\n",
        "        #if it is last cycle, use remaining days as test data\n",
        "        if self.counter_max == self.counter:\n",
        "            TEST_data = self.data.iloc[ -self.last_x_days + self.counter * self.cycle : ]\n",
        "            TRAIN_data = self.data.iloc[ : -self.last_x_days + self.counter * self.cycle ]\n",
        "        else:\n",
        "            TEST_data = self.data.iloc[ -self.last_x_days + self.counter * self.cycle : -self.last_x_days + self.counter * self.cycle + self.cycle ]\n",
        "            TRAIN_data = self.data.iloc[ : -self.last_x_days + self.counter * self.cycle ]\n",
        "\n",
        "\n",
        "        #Create Training set and crash-spike labels\n",
        "        X_train = TRAIN_data.drop( ['Y-Crash', 'Y-Spike'], axis = 1 )\n",
        "        y_train_crash = TRAIN_data[ 'Y-Crash' ]\n",
        "        y_train_spike = TRAIN_data[ 'Y-Spike' ]\n",
        "\n",
        "        #Create test sets\n",
        "        self.X_test = TEST_data.drop( [ 'Y-Spike', 'Y-Crash' ], axis = 1 )\n",
        "        self.y_test_crash = TEST_data[ 'Y-Crash' ]\n",
        "        self.y_test_spike = TEST_data[ 'Y-Spike' ]\n",
        "\n",
        "        #Scale data between 0-1\n",
        "        mm = MinMaxScaler()\n",
        "        mm = mm.fit( X_train )\n",
        "        X_train = mm.transform( X_train )\n",
        "        self.X_test = mm.transform( self.X_test )\n",
        "\n",
        "        #Create samples with smotetomek to eliminate imbalance of positive instances\n",
        "        self.X_train_crash, self.y_train_crash = self.smotetomek( X_train, y_train_crash )\n",
        "        self.X_train_spike, self.y_train_spike = self.smotetomek( X_train, y_train_spike )\n",
        "\n",
        "\n",
        "    def make_pred( self, typez ):\n",
        "        XXX = self.X_test\n",
        "        if typez == 'crash':\n",
        "            TARGET = self.y_test_crash\n",
        "            classifiers = self.class_list_crash\n",
        "            class_keys = self.class_key_crash\n",
        "        else:\n",
        "            TARGET = self.y_test_spike\n",
        "            classifiers = self.class_list_spike\n",
        "            class_keys = self.class_key_spike\n",
        "\n",
        "        for i, classifier in enumerate( classifiers ):\n",
        "            temp = classifier.predict( XXX )\n",
        "            if i == 0:\n",
        "                results = pd.DataFrame( { 'Target': TARGET, class_keys[ i ] : temp } )\n",
        "            else:\n",
        "                results[ class_keys[ i ] ] = temp\n",
        "       \n",
        "\n",
        "        if typez == 'crash':\n",
        "            if self.counter == 0:\n",
        "                self.results_crash = results\n",
        "            else:\n",
        "                self.results_crash = self.results_crash.append( results )\n",
        "\n",
        "        else:\n",
        "            if self.counter == 0:\n",
        "                self.results_spike = results\n",
        "            else:\n",
        "\n",
        "                self.results_crash = self.results_crash.append( results )\n",
        "\n",
        "\n",
        "    def confusion_matrix( self, classifier, key ):\n",
        "        if key == 'crash':\n",
        "            pred = self.results_crash[ classifier ]\n",
        "            true = self.results_crash[ 'Target' ]\n",
        "            return confusion_matrix( true, pred )\n",
        "        elif key == 'spike':\n",
        "            pred = self.results_spike[ classifier ]\n",
        "            true = self.results_spike[ 'Target' ]\n",
        "            return confusion_matrix( true, pred )\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhfFqjOEdSPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tuner( HELP ):\n",
        "    def __init__( self, multicollinearity = False, typez = 'spike' ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.metrics = [ f1_score, precision_score, roc_auc_score ]\n",
        "        self.metircs_key = [ 'f1', 'precision', 'roc_auc']\n",
        "        self.classifier_list = [ ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier,\n",
        "                                GradientBoostingClassifier, SVC, MLPClassifier, KNeighborsClassifier,\n",
        "                                CatBoostClassifier, LGBMClassifier,  XGBClassifier, StackingCVClassifier,\n",
        "                                CostSensitiveRandomForestClassifier ]\n",
        "\n",
        "        self.classifier_list_str = [ 'ExtraTreesClassifier','RandomForestClassifier', 'AdaBoostClassifier',\n",
        "                                'GradientBoostingClassifier', 'SVC', 'MLPClassifier', 'KNeighborsClassifier',\n",
        "                                'CatBoostClassifier', 'LGBMClassifier',  'XGBClassifier', 'StackingCVClassifier',\n",
        "                                'CostSensitiveRandomForestClassifier' ]\n",
        "        self.typez = typez\n",
        "\n",
        "\n",
        "    def scale( self, X_train, X_test, scale_type = 'minmax' ):\n",
        "        if scale_type == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        elif scale_type == 'robust':\n",
        "            scaler = RobustScaler()\n",
        "        elif scale_type == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif scale_type == 'maxabs':\n",
        "            scaler = MaxAbsScaler()\n",
        "\n",
        "        scaler = scaler.fit( X_train )\n",
        "        X_train = scaler.transform( X_train )\n",
        "        X_test = scaler.transform( X_test )\n",
        "        \n",
        "        return X_train, X_test\n",
        "\n",
        "\n",
        "    def prepare_data( self, shuffle = True, smotetomek = True, b_test = False, test_percent = 0.2 ):\n",
        "        \n",
        "        data = self.data\n",
        "            \n",
        "        X = data.drop( ['Y-Crash', 'Y-Spike'], axis = 1 )\n",
        "        if self.typez == 'crash':\n",
        "            y = data[ 'Y-Crash' ]\n",
        "        else:\n",
        "            y = data[ 'Y-Spike' ]\n",
        "       \n",
        "        if b_test:\n",
        "            shuffle = False\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = test_percent, random_state = 42, shuffle = shuffle)\n",
        "\n",
        "        if smotetomek:\n",
        "            X_train, y_train = self.smotetomek( X_train, y_train )\n",
        "\n",
        "        X_train, X_test = self.scale( X_train, X_test )\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "    def process_data( self, return_days = 14, multicollinearity = False,\n",
        "                     shuffle = False, smotetomek = True,\n",
        "                     b_test = False, b_test_size = None,\n",
        "                     test_percent = 0.2 ):\n",
        "        global X_VAL\n",
        "        global Y_VAL\n",
        "      \n",
        "        self.preprocess()\n",
        "        self.define_classes( return_days, multicollinearity )\n",
        "        \n",
        "        X_train, self.X_test, y_train, self.y_test = self.prepare_data( shuffle = shuffle,\n",
        "                                                                        smotetomek = smotetomek,\n",
        "                                                                        b_test = b_test,\n",
        "                                                                        test_percent = test_percent)\n",
        "        \n",
        "        if b_test:\n",
        "            self.X_train = X_train\n",
        "            self.y_train = y_train\n",
        "            self.TEST_RETURNS = self.ALL_RETURNS[ -len( self.y_test ) : ]\n",
        "            self.TEST_DATES = self.DATE_ALL_DATA[ -len( self.y_test ) : ]\n",
        "\n",
        "        else:\n",
        "            self.X_train, X_VAL, self.y_train, Y_VAL = train_test_split( X_train, y_train, test_size = 0.2 )\n",
        "\n",
        "\n",
        "    def print_class_options( self ):\n",
        "        for elem in self.classifier_list_str:\n",
        "            print('{} \\n'.format( elem ) )\n",
        "\n",
        "    def choose_classifier( self, classifier, backtest = False ):\n",
        "        self.class_key = classifier\n",
        "        for ELEM in self.classifier_list_str:\n",
        "            if classifier == ELEM[ : len( classifier )  ]: \n",
        "                if backtest:\n",
        "                    self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]( **self.best_params )\n",
        "                else:\n",
        "                    self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]()\n",
        "\n",
        "    def get_best_params( self ):\n",
        "        self.best_params = self.optimizer.best_params_\n",
        "        print( self.best_params )\n",
        "\n",
        "    def confusion_matrix( self ):\n",
        "        pred = self.optimizer( self.X_test )\n",
        "        true = self.y_test\n",
        "        cm = np.array(confusion_matrix( true, pred ))\n",
        "        plot_confusion_matrix( cm = cm, target_names = [ 'nothing', 'spike' ] )\n",
        "\n",
        "    def tune_classifier( self, args, num_of_iter ):\n",
        "        ff = make_scorer( f1_score, greater_is_better = True )\n",
        "        self.optimizer = BayesSearchCV(\n",
        "                                        self.classifier,\n",
        "                                        args,\n",
        "                                        n_iter = num_of_iter, \n",
        "                                        random_state = 21,\n",
        "                                        scoring = ff,\n",
        "                                        cv = 5\n",
        "                                      )\n",
        "        self.optimizer.fit( self.X_train, self.y_train )\n",
        "\n",
        "    def cash_test( self, preds ):\n",
        "        cash = [ 0 for i in range( 13 ) ]\n",
        "        CASHH = 0\n",
        "        cash_gt = [ 0 for i in range( 13 ) ]\n",
        "        CASHH_GT = 0\n",
        "        test_ret_values = list( self.TEST_RETURNS )\n",
        "        self.y_test = list(self.y_test)\n",
        "        for i in range( len( preds ) ):\n",
        "            CASHH = CASHH + test_ret_values[ i ] * preds[ i ]\n",
        "            cash.append( CASHH )\n",
        "            CASHH_GT = CASHH_GT + test_ret_values[ i ] * preds[ i ]\n",
        "            cash_gt.append( CASHH_GT )\n",
        "        return CASHH, CASHH_GT\n",
        "\n",
        "\n",
        "    def train_tuned( self ):\n",
        "        self.get_best_params()\n",
        "        self.process_data( b_test = True, test_size = 0.1 )\n",
        "        self.choose_classifier( self.class_key, backtest = True )\n",
        "        self.classifier.fit( self.X_train, self.y_train )\n",
        "        preds = self.classifier.predict( self.X_test )\n",
        "        cash, cash_gt = self.cash_test( preds )\n",
        "        return cash, cash_gt\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LnjUmk2PUmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rosemary = Tuner()\n",
        "rosemary.process_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXaW0tptVxoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Displays available classifiers\n",
        "rosemary.print_class_options()\n",
        "#You can choose classifier by specifying first few letter\n",
        "rosemary.choose_classifier( 'Cat' )\n",
        "#Choose parameters to tune\n",
        "tuning_args = {  \n",
        "                'iterations': Integer( 100, 1000, prior='uniform' ),\n",
        "                'learning_rate': Real( 0.0001, 1, prior='uniform' ),\n",
        "                'depth' : Integer( 4, 12, prior='uniform' ),\n",
        "                'border_count': Integer( 1, 200, prior='uniform' ),\n",
        "                'grow_policy' : Categorical( [ 'SymmetricTree', 'Depthwise', 'Lossguide' ] )\n",
        "              }\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5c964MEYBZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e55f4e-eb10-4f84-e454-c06fb9e2555a"
      },
      "source": [
        "rosemary.tune_classifier( tuning_args, num_of_iter = 30 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "151:\tlearn: 0.0017338\ttotal: 59.1s\tremaining: 46.3s\n",
            "152:\tlearn: 0.0017338\ttotal: 59.4s\tremaining: 45.8s\n",
            "153:\tlearn: 0.0017065\ttotal: 59.8s\tremaining: 45.4s\n",
            "154:\tlearn: 0.0017065\ttotal: 1m\tremaining: 45s\n",
            "155:\tlearn: 0.0017065\ttotal: 1m\tremaining: 44.6s\n",
            "156:\tlearn: 0.0017064\ttotal: 1m\tremaining: 44.2s\n",
            "157:\tlearn: 0.0017064\ttotal: 1m 1s\tremaining: 43.8s\n",
            "158:\tlearn: 0.0017063\ttotal: 1m 1s\tremaining: 43.4s\n",
            "159:\tlearn: 0.0017063\ttotal: 1m 1s\tremaining: 42.9s\n",
            "160:\tlearn: 0.0016890\ttotal: 1m 2s\tremaining: 42.5s\n",
            "161:\tlearn: 0.0016890\ttotal: 1m 2s\tremaining: 42.1s\n",
            "162:\tlearn: 0.0016889\ttotal: 1m 2s\tremaining: 41.7s\n",
            "163:\tlearn: 0.0016887\ttotal: 1m 3s\tremaining: 41.3s\n",
            "164:\tlearn: 0.0016887\ttotal: 1m 3s\tremaining: 40.9s\n",
            "165:\tlearn: 0.0016887\ttotal: 1m 4s\tremaining: 40.5s\n",
            "166:\tlearn: 0.0016887\ttotal: 1m 4s\tremaining: 40.1s\n",
            "167:\tlearn: 0.0016887\ttotal: 1m 4s\tremaining: 39.7s\n",
            "168:\tlearn: 0.0016886\ttotal: 1m 5s\tremaining: 39.3s\n",
            "169:\tlearn: 0.0016886\ttotal: 1m 5s\tremaining: 38.9s\n",
            "170:\tlearn: 0.0016885\ttotal: 1m 5s\tremaining: 38.5s\n",
            "171:\tlearn: 0.0016885\ttotal: 1m 6s\tremaining: 38.1s\n",
            "172:\tlearn: 0.0016676\ttotal: 1m 6s\tremaining: 37.7s\n",
            "173:\tlearn: 0.0016675\ttotal: 1m 6s\tremaining: 37.3s\n",
            "174:\tlearn: 0.0016675\ttotal: 1m 7s\tremaining: 36.9s\n",
            "175:\tlearn: 0.0016675\ttotal: 1m 7s\tremaining: 36.5s\n",
            "176:\tlearn: 0.0016674\ttotal: 1m 7s\tremaining: 36.1s\n",
            "177:\tlearn: 0.0016671\ttotal: 1m 8s\tremaining: 35.7s\n",
            "178:\tlearn: 0.0016670\ttotal: 1m 8s\tremaining: 35.3s\n",
            "179:\tlearn: 0.0016670\ttotal: 1m 9s\tremaining: 34.9s\n",
            "180:\tlearn: 0.0016670\ttotal: 1m 9s\tremaining: 34.5s\n",
            "181:\tlearn: 0.0016670\ttotal: 1m 9s\tremaining: 34.1s\n",
            "182:\tlearn: 0.0016669\ttotal: 1m 10s\tremaining: 33.7s\n",
            "183:\tlearn: 0.0016668\ttotal: 1m 10s\tremaining: 33.3s\n",
            "184:\tlearn: 0.0016668\ttotal: 1m 10s\tremaining: 32.9s\n",
            "185:\tlearn: 0.0016668\ttotal: 1m 11s\tremaining: 32.5s\n",
            "186:\tlearn: 0.0016665\ttotal: 1m 11s\tremaining: 32.1s\n",
            "187:\tlearn: 0.0016664\ttotal: 1m 11s\tremaining: 31.7s\n",
            "188:\tlearn: 0.0016664\ttotal: 1m 12s\tremaining: 31.3s\n",
            "189:\tlearn: 0.0016663\ttotal: 1m 12s\tremaining: 30.9s\n",
            "190:\tlearn: 0.0016663\ttotal: 1m 12s\tremaining: 30.5s\n",
            "191:\tlearn: 0.0016145\ttotal: 1m 13s\tremaining: 30.1s\n",
            "192:\tlearn: 0.0016144\ttotal: 1m 13s\tremaining: 29.7s\n",
            "193:\tlearn: 0.0016142\ttotal: 1m 13s\tremaining: 29.3s\n",
            "194:\tlearn: 0.0016141\ttotal: 1m 14s\tremaining: 28.9s\n",
            "195:\tlearn: 0.0016139\ttotal: 1m 14s\tremaining: 28.5s\n",
            "196:\tlearn: 0.0016139\ttotal: 1m 14s\tremaining: 28.2s\n",
            "197:\tlearn: 0.0016139\ttotal: 1m 15s\tremaining: 27.8s\n",
            "198:\tlearn: 0.0016138\ttotal: 1m 15s\tremaining: 27.4s\n",
            "199:\tlearn: 0.0016132\ttotal: 1m 16s\tremaining: 27s\n",
            "200:\tlearn: 0.0016131\ttotal: 1m 16s\tremaining: 26.6s\n",
            "201:\tlearn: 0.0016131\ttotal: 1m 16s\tremaining: 26.2s\n",
            "202:\tlearn: 0.0016129\ttotal: 1m 17s\tremaining: 25.8s\n",
            "203:\tlearn: 0.0016128\ttotal: 1m 17s\tremaining: 25.4s\n",
            "204:\tlearn: 0.0016128\ttotal: 1m 17s\tremaining: 25s\n",
            "205:\tlearn: 0.0016126\ttotal: 1m 18s\tremaining: 24.6s\n",
            "206:\tlearn: 0.0016126\ttotal: 1m 18s\tremaining: 24.2s\n",
            "207:\tlearn: 0.0016126\ttotal: 1m 18s\tremaining: 23.9s\n",
            "208:\tlearn: 0.0016124\ttotal: 1m 19s\tremaining: 23.5s\n",
            "209:\tlearn: 0.0016123\ttotal: 1m 19s\tremaining: 23.1s\n",
            "210:\tlearn: 0.0016122\ttotal: 1m 19s\tremaining: 22.7s\n",
            "211:\tlearn: 0.0016122\ttotal: 1m 20s\tremaining: 22.3s\n",
            "212:\tlearn: 0.0016118\ttotal: 1m 20s\tremaining: 21.9s\n",
            "213:\tlearn: 0.0016116\ttotal: 1m 20s\tremaining: 21.5s\n",
            "214:\tlearn: 0.0016115\ttotal: 1m 21s\tremaining: 21.1s\n",
            "215:\tlearn: 0.0016113\ttotal: 1m 21s\tremaining: 20.8s\n",
            "216:\tlearn: 0.0016113\ttotal: 1m 21s\tremaining: 20.4s\n",
            "217:\tlearn: 0.0016113\ttotal: 1m 22s\tremaining: 20s\n",
            "218:\tlearn: 0.0016112\ttotal: 1m 22s\tremaining: 19.6s\n",
            "219:\tlearn: 0.0016112\ttotal: 1m 23s\tremaining: 19.2s\n",
            "220:\tlearn: 0.0016112\ttotal: 1m 23s\tremaining: 18.9s\n",
            "221:\tlearn: 0.0016110\ttotal: 1m 23s\tremaining: 18.5s\n",
            "222:\tlearn: 0.0016109\ttotal: 1m 24s\tremaining: 18.1s\n",
            "223:\tlearn: 0.0016109\ttotal: 1m 24s\tremaining: 17.7s\n",
            "224:\tlearn: 0.0016109\ttotal: 1m 24s\tremaining: 17.3s\n",
            "225:\tlearn: 0.0016108\ttotal: 1m 25s\tremaining: 17s\n",
            "226:\tlearn: 0.0016106\ttotal: 1m 25s\tremaining: 16.6s\n",
            "227:\tlearn: 0.0016106\ttotal: 1m 25s\tremaining: 16.2s\n",
            "228:\tlearn: 0.0016105\ttotal: 1m 26s\tremaining: 15.8s\n",
            "229:\tlearn: 0.0016105\ttotal: 1m 26s\tremaining: 15.4s\n",
            "230:\tlearn: 0.0016104\ttotal: 1m 26s\tremaining: 15.1s\n",
            "231:\tlearn: 0.0016104\ttotal: 1m 27s\tremaining: 14.7s\n",
            "232:\tlearn: 0.0016103\ttotal: 1m 27s\tremaining: 14.3s\n",
            "233:\tlearn: 0.0016102\ttotal: 1m 28s\tremaining: 13.9s\n",
            "234:\tlearn: 0.0016102\ttotal: 1m 28s\tremaining: 13.5s\n",
            "235:\tlearn: 0.0016101\ttotal: 1m 28s\tremaining: 13.2s\n",
            "236:\tlearn: 0.0016101\ttotal: 1m 29s\tremaining: 12.8s\n",
            "237:\tlearn: 0.0016100\ttotal: 1m 29s\tremaining: 12.4s\n",
            "238:\tlearn: 0.0016099\ttotal: 1m 29s\tremaining: 12s\n",
            "239:\tlearn: 0.0016099\ttotal: 1m 30s\tremaining: 11.6s\n",
            "240:\tlearn: 0.0016097\ttotal: 1m 30s\tremaining: 11.3s\n",
            "241:\tlearn: 0.0016096\ttotal: 1m 30s\tremaining: 10.9s\n",
            "242:\tlearn: 0.0016095\ttotal: 1m 31s\tremaining: 10.5s\n",
            "243:\tlearn: 0.0016094\ttotal: 1m 31s\tremaining: 10.1s\n",
            "244:\tlearn: 0.0016094\ttotal: 1m 31s\tremaining: 9.76s\n",
            "245:\tlearn: 0.0016093\ttotal: 1m 32s\tremaining: 9.38s\n",
            "246:\tlearn: 0.0016093\ttotal: 1m 32s\tremaining: 9.01s\n",
            "247:\tlearn: 0.0016092\ttotal: 1m 33s\tremaining: 8.63s\n",
            "248:\tlearn: 0.0016092\ttotal: 1m 33s\tremaining: 8.26s\n",
            "249:\tlearn: 0.0016091\ttotal: 1m 33s\tremaining: 7.88s\n",
            "250:\tlearn: 0.0016091\ttotal: 1m 34s\tremaining: 7.5s\n",
            "251:\tlearn: 0.0016090\ttotal: 1m 34s\tremaining: 7.13s\n",
            "252:\tlearn: 0.0016090\ttotal: 1m 34s\tremaining: 6.75s\n",
            "253:\tlearn: 0.0016088\ttotal: 1m 35s\tremaining: 6.38s\n",
            "254:\tlearn: 0.0016088\ttotal: 1m 35s\tremaining: 6s\n",
            "255:\tlearn: 0.0016088\ttotal: 1m 35s\tremaining: 5.62s\n",
            "256:\tlearn: 0.0016086\ttotal: 1m 36s\tremaining: 5.24s\n",
            "257:\tlearn: 0.0016085\ttotal: 1m 36s\tremaining: 4.87s\n",
            "258:\tlearn: 0.0016083\ttotal: 1m 37s\tremaining: 4.5s\n",
            "259:\tlearn: 0.0016083\ttotal: 1m 37s\tremaining: 4.12s\n",
            "260:\tlearn: 0.0016083\ttotal: 1m 37s\tremaining: 3.75s\n",
            "261:\tlearn: 0.0016082\ttotal: 1m 38s\tremaining: 3.37s\n",
            "262:\tlearn: 0.0016082\ttotal: 1m 38s\tremaining: 3s\n",
            "263:\tlearn: 0.0016082\ttotal: 1m 38s\tremaining: 2.62s\n",
            "264:\tlearn: 0.0016082\ttotal: 1m 39s\tremaining: 2.25s\n",
            "265:\tlearn: 0.0016080\ttotal: 1m 39s\tremaining: 1.87s\n",
            "266:\tlearn: 0.0016080\ttotal: 1m 39s\tremaining: 1.5s\n",
            "267:\tlearn: 0.0016080\ttotal: 1m 40s\tremaining: 1.12s\n",
            "268:\tlearn: 0.0016079\ttotal: 1m 40s\tremaining: 749ms\n",
            "269:\tlearn: 0.0016079\ttotal: 1m 41s\tremaining: 374ms\n",
            "270:\tlearn: 0.0016078\ttotal: 1m 41s\tremaining: 0us\n",
            "0.9343629343629343\n",
            "0:\tlearn: 0.3875095\ttotal: 551ms\tremaining: 2m 28s\n",
            "1:\tlearn: 0.2710765\ttotal: 968ms\tremaining: 2m 10s\n",
            "2:\tlearn: 0.1737812\ttotal: 1.52s\tremaining: 2m 15s\n",
            "3:\tlearn: 0.1361459\ttotal: 1.99s\tremaining: 2m 12s\n",
            "4:\tlearn: 0.0940387\ttotal: 2.54s\tremaining: 2m 15s\n",
            "5:\tlearn: 0.0710920\ttotal: 3.08s\tremaining: 2m 15s\n",
            "6:\tlearn: 0.0600584\ttotal: 3.52s\tremaining: 2m 12s\n",
            "7:\tlearn: 0.0506945\ttotal: 4.08s\tremaining: 2m 14s\n",
            "8:\tlearn: 0.0388690\ttotal: 4.59s\tremaining: 2m 13s\n",
            "9:\tlearn: 0.0333382\ttotal: 5.07s\tremaining: 2m 12s\n",
            "10:\tlearn: 0.0287284\ttotal: 5.62s\tremaining: 2m 12s\n",
            "11:\tlearn: 0.0233699\ttotal: 6.15s\tremaining: 2m 12s\n",
            "12:\tlearn: 0.0187842\ttotal: 6.79s\tremaining: 2m 14s\n",
            "13:\tlearn: 0.0168112\ttotal: 7.23s\tremaining: 2m 12s\n",
            "14:\tlearn: 0.0142048\ttotal: 7.77s\tremaining: 2m 12s\n",
            "15:\tlearn: 0.0122613\ttotal: 8.3s\tremaining: 2m 12s\n",
            "16:\tlearn: 0.0109597\ttotal: 8.89s\tremaining: 2m 12s\n",
            "17:\tlearn: 0.0094390\ttotal: 9.44s\tremaining: 2m 12s\n",
            "18:\tlearn: 0.0088623\ttotal: 9.88s\tremaining: 2m 11s\n",
            "19:\tlearn: 0.0081327\ttotal: 10.3s\tremaining: 2m 9s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43GU48DTYBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Display best params found\n",
        "rosemary.get_best_params()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xhlyQT3bahU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get test results\n",
        "test_pred = rosemary.optimizer.predict( rosemary.X_test )"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK7oXTZepUTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculate f1 score\n",
        "f1_score( rosemary.y_test, test_pred )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJYGU8EzY9tJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot confusion matrix\n",
        "rosemary.confusion_matrix()\n",
        "\n",
        "#if it does not work, try the code below. \n",
        "#You need to have test_pred first\n",
        "#cm = np.array(confusion_matrix( rosemary.y_test, test_pred ))\n",
        "#plot_confusion_matrix( cm = cm, target_names = [ 'nothing', 'spike' ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giHasdJMY9rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get back-test results\n",
        "pred_returns, returns_with_grand_truth = rosemary.train_tuned()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukxnTC4OaMHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot( pred_returns )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMuWJEOEY9pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eclLQCu4rvOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W_bOrWYsPK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u0H-f3krlv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cCaMnyGXdds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJXDgjw1Y9no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjyxXMp_Y9mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}