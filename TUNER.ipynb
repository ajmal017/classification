{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TUNER.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQtiqwXZU4dV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install yahoofinancials\n",
        "!pip install pandas-ta\n",
        "!pip install catboost\n",
        "!pip install costcla\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEB_fBD1PBWj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "6f50c1d8-84e5-42d3-80df-af8c44980463"
      },
      "source": [
        "#IMPORT CLASSIFIERS\n",
        "#IMPORT CLASSIFIERS\n",
        "from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from costcla.models import CostSensitiveRandomForestClassifier\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "#IMPORT OTHERS\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import scipy.stats as st\n",
        "import pandas_ta as ta\n",
        "import time\n",
        "import warnings\n",
        "import itertools\n",
        "import hyperopt as hp\n",
        "\n",
        "from collections import OrderedDict \n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "from sklearn.metrics import classification_report, roc_auc_score, f1_score, precision_score, recall_score, accuracy_score, plot_confusion_matrix, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from datetime import datetime\n",
        "from yahoofinancials import YahooFinancials\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler, MaxAbsScaler\n",
        "from sklearn.model_selection import train_test_split,  cross_val_score, KFold, cross_val_score\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.combine import SMOTETomek\n",
        "from statsmodels.regression.linear_model import OLS\n",
        "from statsmodels.tools.tools import add_constant\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neural_network.multilayer_perceptron module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neural_network. Anything that cannot be imported from sklearn.neural_network is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hinsS6qAfo_5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# THIS ONE FOR CONFUSION MATRIX\n",
        "def plot_confusion_matrix(cm,\n",
        "                          target_names, \n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=False):\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bEUTB30YJ3E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HELP():\n",
        "    def __init__( self ):\n",
        "        self.classifier_list = [ ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier,\n",
        "                                GradientBoostingClassifier, SVC, MLPClassifier, KNeighborsClassifier,\n",
        "                                CatBoostClassifier, LGBMClassifier,  XGBClassifier, StackingCVClassifier,\n",
        "                                CostSensitiveRandomForestClassifier ]\n",
        "\n",
        "        self.classifier_list_str = [ 'ExtraTreesClassifier','RandomForestClassifier', 'AdaBoostClassifier',\n",
        "                                'GradientBoostingClassifier', 'SVC', 'MLPClassifier', 'KNeighborsClassifier',\n",
        "                                'CatBoostClassifier', 'LGBMClassifier',  'XGBClassifier', 'StackingCVClassifier',\n",
        "                                'CostSensitiveRandomForestClassifier' ]\n",
        "\n",
        "        #self.class_list_spike = list()\n",
        "        #self.class_list_crash = list()\n",
        "        #self.class_key_spike = list()\n",
        "        #self.class_key_crash = list()\n",
        "        #self.last_x_days = last_x_days\n",
        "        #self.cycle = cycle\n",
        "        #self.counter = 0\n",
        "        #self.counter_max = last_x_days // cycle -1 if last_x_days % cycle == 0 else last_x_days // cycle \n",
        "        #self.last_cycle = self.cycle if last_x_days % cycle == 0 else last_x_days % cycle\n",
        "        #self.preprocess()\n",
        "        #self.define_classes( return_days, multicollinearity )\n",
        "        #self.DATE_TEST = self.DATE_ALL_DATA.iloc[ -last_x_days : ]\n",
        "        #self.TEST_RETURNS = self.ALL_RETURNS[ -last_x_days : ]\n",
        "\n",
        "    def preprocess( self ):\n",
        "        print('\\n PREPROCESSING \\n')\n",
        "\n",
        "        ticker_details = pd.read_excel('Ticker List.xlsx')\n",
        "        ticker = ticker_details['Ticker'].to_list()\n",
        "        names = ticker_details['Description'].to_list()\n",
        "\n",
        "        #Extracting Data from Yahoo Finance and Adding them to Values table using date as key\n",
        "        end_date= \"2020-06-19\"\n",
        "        start_date = \"2000-01-01\"\n",
        "        date_range = pd.bdate_range(start=start_date,end=end_date)\n",
        "        values = pd.DataFrame({ 'Date': date_range})\n",
        "        values['Date']= pd.to_datetime(values['Date'])\n",
        "\n",
        "        #Extracting Data from Yahoo Finance and Adding them to Values table using date as key\n",
        "        for i in ticker:\n",
        "            raw_data = YahooFinancials(i)\n",
        "            raw_data = raw_data.get_historical_price_data(start_date, end_date, \"daily\")\n",
        "            df = pd.DataFrame(raw_data[i]['prices'])[['formatted_date','adjclose']]\n",
        "            df.columns = ['Date1',i]\n",
        "            df['Date1']= pd.to_datetime(df['Date1'])\n",
        "            values = values.merge(df,how='left',left_on='Date',right_on='Date1')\n",
        "            values = values.drop(labels='Date1',axis=1)\n",
        "        self.VV = values\n",
        "\n",
        "        #Renaming columns to represent instrument names rather than their ticker codes for ease of readability\n",
        "        names.insert(0,'Date')\n",
        "        values.columns = names\n",
        "        values.tail()\n",
        "\n",
        "\n",
        "        #Front filling the NaN values in the data set\n",
        "        values = values.fillna(method=\"ffill\",axis=0)\n",
        "        values = values.fillna(method=\"bfill\",axis=0)\n",
        "        values.isna().sum()\n",
        "\n",
        "        #Return\n",
        "        values['SPX-RSI'] = ta.rsi( values['SPX'] )\n",
        "\n",
        "        BBANDS = ta.bbands( values['SPX'] )\n",
        "        keys = BBANDS.keys().to_list()\n",
        "\n",
        "        Upper = BBANDS[ 'BBU_5' ]\n",
        "        Lower = BBANDS[ 'BBL_5' ]\n",
        "\n",
        "        Upper_perc = Upper / values['SPX']\n",
        "        Lower_perc = Lower / values['SPX']\n",
        "\n",
        "        values[ 'BBU-Distance' ] = Upper_perc\n",
        "        values[ 'BBL-Distance' ] = Lower_perc\n",
        "        values['MACD-Histogram'] = ta.macd( values[ 'SPX' ] )[ 'MACDH_12_26_9' ]\n",
        "        # Co-ercing numeric type to all columns except Date\n",
        "        cols=values.columns.drop('Date')\n",
        "        values[cols] = values[cols].apply(pd.to_numeric,errors='coerce').round(decimals=4)\n",
        "        #print(values.tail())\n",
        "\n",
        "        imp = ['Gold','USD Index', 'Oil', 'SPX','VIX', 'High Yield Fund' , 'Nikkei', 'Dax', '10Yr', '2Yr' , 'EEM' ,'XLE', 'XLF', 'XLI', 'AUDJPY']\n",
        "        # Calculating Short term -Historical Returns\n",
        "        change_days = [1,3,5,14,21]\n",
        "        \n",
        "        data = pd.DataFrame(data=values['Date'])\n",
        "        for i in change_days:\n",
        "            x= values[cols].pct_change(periods=i).add_suffix(\"-T-\"+str(i))\n",
        "            data=pd.concat(objs=(data,x),axis=1)\n",
        "            x=[]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculating Long term Historical Returns\n",
        "        change_days = [60,90,180,250]\n",
        "\n",
        "        for i in change_days:\n",
        "            x= values[imp].pct_change(periods=i).add_suffix(\"-T-\"+str(i))\n",
        "            data=pd.concat(objs=(data,x),axis=1)\n",
        "            x=[]\n",
        "\n",
        "        #Calculating Moving averages for SPX\n",
        "        moving_avg = pd.DataFrame(values['Date'],columns=['Date'])\n",
        "        moving_avg['Date']=pd.to_datetime(moving_avg['Date'],format='%Y-%b-%d')\n",
        "        moving_avg['SPX/15SMA'] = (values['SPX']/(values['SPX'].rolling(window=15).mean()))-1\n",
        "        moving_avg['SPX/30SMA'] = (values['SPX']/(values['SPX'].rolling(window=30).mean()))-1\n",
        "        moving_avg['SPX/60SMA'] = (values['SPX']/(values['SPX'].rolling(window=60).mean()))-1\n",
        "        moving_avg['SPX/90SMA'] = (values['SPX']/(values['SPX'].rolling(window=90).mean()))-1\n",
        "        moving_avg['SPX/180SMA'] = (values['SPX']/(values['SPX'].rolling(window=180).mean()))-1\n",
        "        moving_avg['SPX/90EMA'] = (values['SPX']/(values['SPX'].ewm(span=90,adjust=True,ignore_na=True).mean()))-1\n",
        "        moving_avg['SPX/180EMA'] = (values['SPX']/(values['SPX'].ewm(span=180,adjust=True,ignore_na=True).mean()))-1\n",
        "        moving_avg = moving_avg.dropna(axis=0)\n",
        "        #Merging Moving Average values to the feature space\n",
        "        data['Date']=pd.to_datetime(data['Date'],format='%Y-%b-%d')\n",
        "\n",
        "\n",
        "        self.RAW_data = pd.merge(left=data,right=moving_avg,how='left',on='Date')\n",
        "\n",
        "        self.RAW_y = pd.DataFrame(data=values['Date'])\n",
        "\n",
        "        self.RAW_values = values\n",
        "\n",
        "    def mult( self, data, threshold, TARGET = 'SPX' ):\n",
        "        keys = data.keys().to_list()\n",
        "        corr = data.corr()\n",
        "        target_set = corr[ TARGET ]\n",
        "        drop_list = list()\n",
        "        \n",
        "        #loop over features in corr matrix except SPX\n",
        "        for i, key in enumerate(keys):\n",
        "            if key != TARGET:\n",
        "                \n",
        "                print('EXAMINING {} \\n '.format( key ) )\n",
        "                print('DROP_LIST {} \\n'.format( drop_list ) )\n",
        "                #Get 'abs' corr values of current feature \n",
        "                values = corr[ key ]\n",
        "                \n",
        "                #we wont control corr between feature and [ SPX, features itself ] \n",
        "                #we will also skip dropped features in the next loop\n",
        "                skip_list = [ TARGET, key ]\n",
        "\n",
        "                #since skip_list will be updated for each feature\n",
        "                #we save dropped features in drop_list to append it \n",
        "                #while exploring each feature\n",
        "                if len( drop_list ) > 0:\n",
        "                    #drop if drop_list is not empty\n",
        "                    skip_list.extend( drop_list )\n",
        "\n",
        "                print('SKIP LIST {} \\n'.format( skip_list ) )\n",
        "                #we will loop over \n",
        "                for k, ind in enumerate(keys):\n",
        "                    #skip features in skip_list\n",
        "                    if ind not in skip_list:\n",
        "\n",
        "                        print(ind)\n",
        "                        print(ind)\n",
        "                        #if corr between 2 features exceeds threshold\n",
        "                        if abs( values[ k ] ) > threshold:\n",
        "                            print( values[ k ] )\n",
        "                            #check which one is more corr to target\n",
        "                            if corr[ TARGET ][ key ] > corr[ TARGET ][ ind ]:\n",
        "                                #drop the other one\n",
        "                                data = data.drop( ind, axis = 1 )\n",
        "                                #append dropped feature's key to drop_list\n",
        "                                drop_list.append( ind )\n",
        "                                print('DROPPED {} \\n '.format( ind ))\n",
        "                            elif key not in drop_list:\n",
        "                                #inverse of the 'statement' above\n",
        "                                data = data.drop( key, axis = 1 )\n",
        "                                drop_list.append( key )\n",
        "                                print('DROPPED {} \\n '.format( key ))\n",
        "                                break\n",
        "\n",
        "        return data, drop_list\n",
        "                    \n",
        "            \n",
        "    def define_classes( self, return_days, multicollinearity ):\n",
        "        print('\\n DEFINING CLASSES \\n')\n",
        "        y = self.RAW_y\n",
        "        values = self.RAW_values\n",
        "        data = self.RAW_data\n",
        "        \n",
        "        y[ 'SPX-Target' ] = values[ \"SPX\" ].pct_change( periods = -return_days )\n",
        "        y.isna().sum()\n",
        "\n",
        "        # Removing NAs\n",
        "        data = data[ data[ 'SPX-T-250' ].notna() ]\n",
        "        y = y[ y[ 'SPX-Target' ].notna() ]\n",
        "\n",
        "        #Adding Target Variables\n",
        "        data = pd.merge(left=data,right=y,how='inner',on='Date',suffixes=(False,False))\n",
        "        data.isna().sum()\n",
        "        \n",
        "        #Select Threshold p (left tail probability)\n",
        "        p1 = 0.25\n",
        "        p2 = 0.75\n",
        "        #Get z-Value\n",
        "        z1 = st.norm.ppf( p1 )\n",
        "        z2 = st.norm.ppf( p2 )\n",
        "       \n",
        "        #Calculating Threshold (t) for each Y\n",
        "        crash = round( (z1 * np.std( data[ \"SPX-Target\" ] ) ) + np.mean( data[ \"SPX-Target\" ] ), 5 )\n",
        "        spike = round( (z2 * np.std( data[ \"SPX-Target\" ] ) ) + np.mean( data[ \"SPX-Target\" ] ), 5 )\n",
        "        \n",
        "        #Creating Labels\n",
        "        data[ 'Y-Crash' ] = ( data[ 'SPX-Target' ] < crash ) * 1\n",
        "        data[ 'Y-Spike' ] = ( data[ 'SPX-Target' ] > spike ) * 1\n",
        "\n",
        "        #Save Dates\n",
        "        self.DATE_ALL_DATA = data[ 'Date' ]\n",
        "        last_values = values[ values[ 'Date' ] <= data[ 'Date' ].iloc[ -1 ] ]\n",
        "        self.ALL_RETURNS = list()\n",
        "\n",
        "        #Calculate cash returns\n",
        "        RATES = data[ 'SPX-Target' ].to_list()\n",
        "        CLOSE = last_values[ 'SPX' ].to_list()\n",
        "\n",
        "        for i in range( len( data[ 'SPX-Target' ] ) ):\n",
        "            self.ALL_RETURNS.append( RATES[ i ] * CLOSE[ i ] )\n",
        "\n",
        "        #Prepare df for vif calculation\n",
        "        no_target_data =  data.drop( [ 'SPX-Target', 'Date', 'Y-Crash', 'Y-Spike'  ], axis = 1 )\n",
        "        #calculate vif\n",
        "        self.vif_scores = self.variance_inflation_factors( no_target_data )\n",
        "        #get feature names which vif > 10\n",
        "        inds = (self.vif_scores > 10 ).index\n",
        "        droplist = inds[ self.vif_scores > 10 ]\n",
        "\n",
        "        if multicollinearity == True:\n",
        "            data = data.drop( droplist, axis = 1 )\n",
        "\n",
        "        #For now, data has Spike and Crash labels \n",
        "        self.data = data.drop( [ 'SPX-Target', 'Date'], axis = 1 )\n",
        "        \n",
        "\n",
        "    def variance_inflation_factors( self, exog_df ):\n",
        "\n",
        "        exog_df = add_constant(exog_df)\n",
        "        vifs = pd.Series(\n",
        "            [1 / (1. - OLS(exog_df[col].values, \n",
        "                          exog_df.loc[:, exog_df.columns != col].values).fit().rsquared) \n",
        "            for col in exog_df],\n",
        "            index=exog_df.columns,\n",
        "            name='VIF'\n",
        "        )\n",
        "        return vifs\n",
        "\n",
        "\n",
        "    def select_classifier( self, classifier, key ):\n",
        "        #for ELEM in self.classifier_list_str:\n",
        "        #    if classifier == ELEM[ : len( classifier )  ]: \n",
        "        #        self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ) ]\n",
        "        if key == 'crash':\n",
        "            if classifier == 'EXT':\n",
        "                self.class_list_crash.append( ExtraTreesClassifier( n_estimators = 300, random_state = 0 ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "            elif classifier == 'GB':\n",
        "                self.class_list_crash.append( GradientBoostingClassifier( random_state = 0 ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "                    \n",
        "            elif classifier == 'CB':\n",
        "                self.class_list_crash.append( CatBoostClassifier() )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "            elif classifier == 'STCK':\n",
        "                est_list = list()\n",
        "                for i in range( len( self.class_list_crash ) ):\n",
        "                    est_list.append( ( self.class_key_crash[ i ], self.class_list_crash[ i ]  ) )\n",
        "                self.class_list_crash.append( StackingClassifier(estimators = est_list,  cv = 5, final_estimator = CatBoostClassifier() ) )\n",
        "                if classifier not in self.class_key_crash:\n",
        "                    self.class_key_crash.append( classifier )\n",
        "\n",
        "        elif key == 'spike':\n",
        "            if classifier == 'EXT':\n",
        "                self.class_list_spike.append( ExtraTreesClassifier( n_estimators = 300, random_state = 0 ) )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'GB':\n",
        "                self.class_list_spike.append( GradientBoostingClassifier( random_state = 0 ) )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'CB':\n",
        "                self.class_list_spike.append( CatBoostClassifier() )\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "            elif classifier == 'STCK':\n",
        "                est_list = list()\n",
        "                for i in range( len(self.class_list_spike) ):\n",
        "                    est_list.append( ( self.class_key_spike[ i ], self.class_list_spike[ i ]  ) )\n",
        "                self.class_list_spike.append( StackingClassifier( estimators = est_list,  cv = 5, final_estimator = CatBoostClassifier() ) ) \n",
        "\n",
        "                if classifier not in self.class_key_spike:\n",
        "                    self.class_key_spike.append( classifier )\n",
        "\n",
        "    def backtest(self):\n",
        "        list_of_cash_tt = list()\n",
        "        self.make_pred( for_valid = False )\n",
        "        classifiers = self.BT_RESULTS.keys().to_list()\n",
        "        count = 0\n",
        "        for CCC in classifiers:\n",
        "            if CCC != 'Target':\n",
        "                temp_cash = list()\n",
        "                temp_cash.append( 0 )\n",
        "                cc = 0\n",
        "\n",
        "                preds = self.BT_RESULTS[ CCC ].to_list()\n",
        "\n",
        "                assert len( preds ) ==  len( self.TEST_RETURNS )\n",
        "\n",
        "                for i in range( len( preds ) ):\n",
        "                    if preds[ i ] == 1:\n",
        "                        cc += -self.TEST_RETURNS[ i ]\n",
        "                    temp_cash.append( cc )\n",
        "\n",
        "                if count == 0:\n",
        "                    CASH = pd.DataFrame( { CCC: temp_cash  })\n",
        "                    count += 1\n",
        "                else:\n",
        "                    CASH[ CCC ] = temp_cash\n",
        "                    count += 1\n",
        "\n",
        "        self.CASH = CASH\n",
        "      \n",
        "\n",
        "\n",
        "    def scores( self, typez ):\n",
        "        if typez == 'crash':\n",
        "            results = self.results_crash\n",
        "        else:\n",
        "            results = self.results_spike\n",
        "\n",
        "        KEYS = results.keys().to_list()\n",
        "        counter = 0\n",
        "        for  key in KEYS:\n",
        "            if key != 'Target':\n",
        "                score_list = list()\n",
        "                for metric in self.metrics:\n",
        "                    score_list.append( metric( results[ 'Target' ], results[ key ] ) )\n",
        "\n",
        "                if counter == 0:\n",
        "                    scores = pd.DataFrame( { key: score_list  })\n",
        "                    counter = counter + 1\n",
        "                else:\n",
        "                    scores[ key ] = score_list\n",
        "                    counter = counter + 1\n",
        "\n",
        "        if typez == 'crash':\n",
        "            self.scores_crash = scores\n",
        "        else:\n",
        "            self.scores_spike = scores\n",
        "\n",
        "\n",
        "\n",
        "    def redefine_classifiers( self ):\n",
        "        self.class_list_spike = list()\n",
        "        self.class_list_crash = list()\n",
        "        for classifier in self.class_key_spike:\n",
        "            self.select_classifier( classifier, 'spike' )\n",
        "        for classifier in self.class_key_crash:\n",
        "            self.select_classifier( classifier, 'crash' )\n",
        "\n",
        "\n",
        "    def retrain( self ):\n",
        "\n",
        "        for i in range( self.counter_max + 1 ):\n",
        "            start_time = time.time()\n",
        "            print( '\\n TRAIN # {} \\n'.format( i ) )\n",
        "            self.update_data()\n",
        "            self.redefine_classifiers()\n",
        "            for classifier in self.class_list_spike:\n",
        "                classifier.fit( self.X_train_spike, self.y_train_spike )\n",
        "\n",
        "            for classifier in self.class_list_crash:\n",
        "                classifier.fit( self.X_train_crash, self.y_train_crash )\n",
        "            print('FINISHED {} \\n '.format( i ) )\n",
        "            print('Lasts : {} \\n' .format(time.time() - start_time))\n",
        "\n",
        "            self.make_pred( typez = 'crash' )\n",
        "            self.make_pred( typez = 'spike' )\n",
        "            self.counter += 1\n",
        "\n",
        "\n",
        "    def smotetomek( self, X_train, y_train ):\n",
        "        smt = SMOTETomek( random_state = 21 )\n",
        "        return smt.fit_sample( X_train, y_train )\n",
        "\n",
        "\n",
        "    def update_data( self ):\n",
        "\n",
        "        #Update train-test split after every cycle time\n",
        "        #if it is last cycle, use remaining days as test data\n",
        "        if self.counter_max == self.counter:\n",
        "            TEST_data = self.data.iloc[ -self.last_x_days + self.counter * self.cycle : ]\n",
        "            TRAIN_data = self.data.iloc[ : -self.last_x_days + self.counter * self.cycle ]\n",
        "        else:\n",
        "            TEST_data = self.data.iloc[ -self.last_x_days + self.counter * self.cycle : -self.last_x_days + self.counter * self.cycle + self.cycle ]\n",
        "            TRAIN_data = self.data.iloc[ : -self.last_x_days + self.counter * self.cycle ]\n",
        "\n",
        "\n",
        "        #Create Training set and crash-spike labels\n",
        "        X_train = TRAIN_data.drop( ['Y-Crash', 'Y-Spike'], axis = 1 )\n",
        "        y_train_crash = TRAIN_data[ 'Y-Crash' ]\n",
        "        y_train_spike = TRAIN_data[ 'Y-Spike' ]\n",
        "\n",
        "        #Create test sets\n",
        "        self.X_test = TEST_data.drop( [ 'Y-Spike', 'Y-Crash' ], axis = 1 )\n",
        "        self.y_test_crash = TEST_data[ 'Y-Crash' ]\n",
        "        self.y_test_spike = TEST_data[ 'Y-Spike' ]\n",
        "\n",
        "        #Scale data between 0-1\n",
        "        mm = MinMaxScaler()\n",
        "        mm = mm.fit( X_train )\n",
        "        X_train = mm.transform( X_train )\n",
        "        self.X_test = mm.transform( self.X_test )\n",
        "\n",
        "        #Create samples with smotetomek to eliminate imbalance of positive instances\n",
        "        self.X_train_crash, self.y_train_crash = self.smotetomek( X_train, y_train_crash )\n",
        "        self.X_train_spike, self.y_train_spike = self.smotetomek( X_train, y_train_spike )\n",
        "\n",
        "\n",
        "    def make_pred( self, typez ):\n",
        "        XXX = self.X_test\n",
        "        if typez == 'crash':\n",
        "            TARGET = self.y_test_crash\n",
        "            classifiers = self.class_list_crash\n",
        "            class_keys = self.class_key_crash\n",
        "        else:\n",
        "            TARGET = self.y_test_spike\n",
        "            classifiers = self.class_list_spike\n",
        "            class_keys = self.class_key_spike\n",
        "\n",
        "        for i, classifier in enumerate( classifiers ):\n",
        "            temp = classifier.predict( XXX )\n",
        "            if i == 0:\n",
        "                results = pd.DataFrame( { 'Target': TARGET, class_keys[ i ] : temp } )\n",
        "            else:\n",
        "                results[ class_keys[ i ] ] = temp\n",
        "       \n",
        "\n",
        "        if typez == 'crash':\n",
        "            if self.counter == 0:\n",
        "                self.results_crash = results\n",
        "            else:\n",
        "                self.results_crash = self.results_crash.append( results )\n",
        "\n",
        "        else:\n",
        "            if self.counter == 0:\n",
        "                self.results_spike = results\n",
        "            else:\n",
        "\n",
        "                self.results_crash = self.results_crash.append( results )\n",
        "\n",
        "\n",
        "    def confusion_matrix( self, classifier, key ):\n",
        "        if key == 'crash':\n",
        "            pred = self.results_crash[ classifier ]\n",
        "            true = self.results_crash[ 'Target' ]\n",
        "            return confusion_matrix( true, pred )\n",
        "        elif key == 'spike':\n",
        "            pred = self.results_spike[ classifier ]\n",
        "            true = self.results_spike[ 'Target' ]\n",
        "            return confusion_matrix( true, pred )\n",
        "\n",
        "\n",
        "    def cash_test( self, preds ):\n",
        "        cash = [ 0 for i in range( 13 ) ]\n",
        "        CASHH = 0\n",
        "        cash_gt = [ 0 for i in range( 13 ) ]\n",
        "        CASHH_GT = 0\n",
        "        test_ret_values = list( self.TEST_RETURNS )\n",
        "        self.y_test = list(self.y_test)\n",
        "        for i in range( len( preds ) ):\n",
        "            CASHH = CASHH + test_ret_values[ i ] * preds[ i ]\n",
        "            cash.append( CASHH )\n",
        "            CASHH_GT = CASHH_GT + test_ret_values[ i ] * self.y_test[ i ]\n",
        "            cash_gt.append( CASHH_GT )\n",
        "        return cash, cash_gt\n",
        "\n",
        "\n",
        "    def confusion_matrix( self ):\n",
        "        pred = self.optimizer( self.X_test )\n",
        "        true = self.y_test\n",
        "        cm = np.array(confusion_matrix( true, pred ))\n",
        "        plot_confusion_matrix( cm = cm, target_names = [ 'nothing', 'spike' ] )\n",
        "\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhfFqjOEdSPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tuner( HELP ):\n",
        "    def __init__( self, multicollinearity = False, typez = 'spike' ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.metrics = [ f1_score, precision_score, roc_auc_score ]\n",
        "        self.metircs_key = [ 'f1', 'precision', 'roc_auc']\n",
        "        self.classifier_list = [ ExtraTreesClassifier,RandomForestClassifier, AdaBoostClassifier,\n",
        "                                GradientBoostingClassifier, SVC, MLPClassifier, KNeighborsClassifier,\n",
        "                                CatBoostClassifier, LGBMClassifier,  XGBClassifier, StackingCVClassifier,\n",
        "                                CostSensitiveRandomForestClassifier ]\n",
        "\n",
        "        self.classifier_list_str = [ 'ExtraTreesClassifier','RandomForestClassifier', 'AdaBoostClassifier',\n",
        "                                'GradientBoostingClassifier', 'SVC', 'MLPClassifier', 'KNeighborsClassifier',\n",
        "                                'CatBoostClassifier', 'LGBMClassifier',  'XGBClassifier', 'StackingCVClassifier',\n",
        "                                'CostSensitiveRandomForestClassifier' ]\n",
        "        self.typez = typez\n",
        "\n",
        "    def choose_classifier( self, classifier, backtest = False ):\n",
        "        self.class_key = classifier\n",
        "        for ELEM in self.classifier_list_str:\n",
        "            if classifier == ELEM[ : len( classifier )  ]: \n",
        "                if backtest:\n",
        "                    self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]( **self.best_params )\n",
        "                else:\n",
        "                    self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]()\n",
        "\n",
        "\n",
        "    def classifier_to_tune( self, classifier, backtest = False ):\n",
        "        self.class_key = classifier\n",
        "        for ELEM in self.classifier_list_str:\n",
        "            if classifier == ELEM[ : len( classifier )  ]: \n",
        "                self.classifier = self.classifier_list[ self.classifier_list_str.index( ELEM ) ]\n",
        "\n",
        "\n",
        "    def scale( self, X_train, X_test, scale_type = 'minmax' ):\n",
        "        if scale_type == 'minmax':\n",
        "            scaler = MinMaxScaler()\n",
        "        elif scale_type == 'robust':\n",
        "            scaler = RobustScaler()\n",
        "        elif scale_type == 'standard':\n",
        "            scaler = StandardScaler()\n",
        "        elif scale_type == 'maxabs':\n",
        "            scaler = MaxAbsScaler()\n",
        "\n",
        "        scaler = scaler.fit( X_train )\n",
        "        X_train = scaler.transform( X_train )\n",
        "        X_test = scaler.transform( X_test )\n",
        "        \n",
        "        return X_train, X_test\n",
        "\n",
        "\n",
        "    def prepare_data( self, smotetomek = True, b_test = False, test_percent = 0.3 ):\n",
        "        shuffle = True\n",
        "        data = self.data\n",
        "            \n",
        "        X = data.drop( ['Y-Crash', 'Y-Spike'], axis = 1 )\n",
        "\n",
        "        if self.typez == 'crash':\n",
        "            y = data[ 'Y-Crash' ]\n",
        "        else:\n",
        "            y = data[ 'Y-Spike' ]\n",
        "\n",
        "        stratify = y\n",
        "\n",
        "        if b_test:\n",
        "            shuffle = False\n",
        "            stratify = None\n",
        "        \n",
        "        X_train, X_test, y_train, y_test = train_test_split( X, y, test_size = test_percent, random_state = 42, shuffle = shuffle, stratify = stratify )\n",
        "\n",
        "        if smotetomek:\n",
        "            X_train, y_train = self.smotetomek( X_train, y_train )\n",
        "\n",
        "        X_train, X_test = self.scale( X_train, X_test )\n",
        "        \n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "    def process_data(self, return_days = 14, multicollinearity = False,\n",
        "                     smotetomek = True, b_test = False, test_percent = 0.2 ):\n",
        "      \n",
        "        self.preprocess()\n",
        "        self.define_classes( return_days, multicollinearity )\n",
        "        \n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = self.prepare_data( smotetomek = smotetomek,\n",
        "                                                                                  b_test = b_test,\n",
        "                                                                                  test_percent = test_percent)\n",
        "        self.TEST_RETURNS = self.ALL_RETURNS[ -len( self.y_test ) : ]\n",
        "        self.TEST_DATES = self.DATE_ALL_DATA[ -len( self.y_test ) : ]\n",
        "\n",
        "\n",
        "    def print_class_options( self ):\n",
        "        for elem in self.classifier_list_str:\n",
        "            print('{} \\n'.format( elem ) )\n",
        "\n",
        "\n",
        "    def train_tuned( self ):\n",
        "        self.process_data( b_test = True, test_percent = 0.1 )\n",
        "        self.classifier.fit( self.X_train, self.y_train )\n",
        "        preds = self.classifier.predict( self.X_test )\n",
        "        cash, cash_gt = self.cash_test( preds )\n",
        "        return cash, cash_gt\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzj4nDfLWWld",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class HPOpt(Tuner):\n",
        "\n",
        "    def __init__(self, rosemary ):\n",
        "        self.X_train = rosemary.X_train\n",
        "        self.X_test  = rosemary.X_test\n",
        "        self.y_train = rosemary.y_train\n",
        "        self.y_test  = rosemary.y_test\n",
        "        self.classifier = rosemary.classifier\n",
        "\n",
        "    def process(self, fn_name, space, trials, algo, max_evals):\n",
        "        \n",
        "        fn = getattr(self, fn_name)\n",
        "        result = fmin(fn=fn, space=space, algo=algo, max_evals=max_evals, trials=trials)\n",
        "\n",
        "        return result, trials\n",
        "\n",
        "\n",
        "    def objective( self, args ):\n",
        "        clf = self.classifier( **args )\n",
        "        clf.fit( self.X_train, self.y_train )\n",
        "        pred = clf.predict( self.X_test )\n",
        "        score = f1_score( self.y_test, pred )\n",
        "        return {'loss': -score, 'status': STATUS_OK}\n",
        "        "
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LnjUmk2PUmN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "a720d331-4c29-4f8a-eea1-9adf25392264"
      },
      "source": [
        "rosemary = Tuner()\n",
        "rosemary.process_data()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " PREPROCESSING \n",
            "\n",
            "\n",
            " DEFINING CLASSES \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEhw_OvvgNbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rosemary.classifier_to_tune( 'Cat' )"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0HrKFXY9Mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#We define list of values if we'll use hp.choice\n",
        "#Because results just tell us the order of the optimal value\n",
        "iter_list = range(100, 1000)\n",
        "depth_list = range(4, 12)\n",
        "policy_list = [ 'SymmetricTree', 'Depthwise', 'Lossguide' ]\n",
        "border_list = range( 1, 200 )\n",
        "\n",
        "SPACE = OrderedDict([('iterations', hp.choice('iterations', iter_list )  ),\n",
        "                    ('learning_rate', hp.uniform('learning_rate', 0.001, 1 ) ),\n",
        "                    ('depth', hp.choice('depth', depth_list ) ),\n",
        "                    ('border_count', hp.choice('border_count', border_list ) ),\n",
        "                    ('grow_policy', hp.choice('grow_policy', policy_list ) ) ] )\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqaRud9aZDyZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daedc222-697b-4f55-c368-bbb4f3c38111"
      },
      "source": [
        "hhh = HPOpt( rosemary )\n",
        "best_params, trials_obj = hhh.process( fn_name = 'objective', space = SPACE, trials = Trials(), algo = tpe.suggest, max_evals = 100 ) #max_evals will be increased\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "387:\tlearn: 0.0109032\ttotal: 6.38s\tremaining: 7.39s\n",
            "\n",
            "388:\tlearn: 0.0108459\ttotal: 6.4s\tremaining: 7.37s\n",
            "\n",
            "389:\tlearn: 0.0108072\ttotal: 6.42s\tremaining: 7.36s\n",
            "\n",
            "390:\tlearn: 0.0107127\ttotal: 6.44s\tremaining: 7.34s\n",
            "\n",
            "391:\tlearn: 0.0106471\ttotal: 6.45s\tremaining: 7.32s\n",
            "\n",
            "392:\tlearn: 0.0105560\ttotal: 6.47s\tremaining: 7.31s\n",
            "\n",
            "393:\tlearn: 0.0105038\ttotal: 6.48s\tremaining: 7.29s\n",
            "\n",
            "394:\tlearn: 0.0104661\ttotal: 6.5s\tremaining: 7.27s\n",
            "\n",
            "395:\tlearn: 0.0103997\ttotal: 6.51s\tremaining: 7.25s\n",
            "\n",
            "396:\tlearn: 0.0103530\ttotal: 6.53s\tremaining: 7.24s\n",
            "\n",
            "397:\tlearn: 0.0102826\ttotal: 6.55s\tremaining: 7.22s\n",
            "\n",
            "398:\tlearn: 0.0102270\ttotal: 6.56s\tremaining: 7.2s\n",
            "\n",
            "399:\tlearn: 0.0101740\ttotal: 6.58s\tremaining: 7.19s\n",
            "\n",
            "400:\tlearn: 0.0101351\ttotal: 6.59s\tremaining: 7.17s\n",
            "\n",
            "401:\tlearn: 0.0100797\ttotal: 6.62s\tremaining: 7.16s\n",
            "\n",
            "402:\tlearn: 0.0099690\ttotal: 6.63s\tremaining: 7.14s\n",
            "\n",
            "403:\tlearn: 0.0099290\ttotal: 6.65s\tremaining: 7.13s\n",
            "\n",
            "404:\tlearn: 0.0098924\ttotal: 6.66s\tremaining: 7.11s\n",
            "\n",
            "405:\tlearn: 0.0097865\ttotal: 6.68s\tremaining: 7.09s\n",
            "\n",
            "406:\tlearn: 0.0097141\ttotal: 6.7s\tremaining: 7.08s\n",
            "\n",
            "407:\tlearn: 0.0096814\ttotal: 6.71s\tremaining: 7.06s\n",
            "\n",
            "408:\tlearn: 0.0096169\ttotal: 6.73s\tremaining: 7.04s\n",
            "\n",
            "409:\tlearn: 0.0095765\ttotal: 6.75s\tremaining: 7.03s\n",
            "\n",
            "410:\tlearn: 0.0095281\ttotal: 6.76s\tremaining: 7.01s\n",
            "\n",
            "411:\tlearn: 0.0094779\ttotal: 6.78s\tremaining: 7s\n",
            "\n",
            "412:\tlearn: 0.0094215\ttotal: 6.8s\tremaining: 6.98s\n",
            "\n",
            "413:\tlearn: 0.0093737\ttotal: 6.81s\tremaining: 6.96s\n",
            "\n",
            "414:\tlearn: 0.0093365\ttotal: 6.82s\tremaining: 6.94s\n",
            "\n",
            "415:\tlearn: 0.0092974\ttotal: 6.84s\tremaining: 6.92s\n",
            "\n",
            "416:\tlearn: 0.0092406\ttotal: 6.86s\tremaining: 6.91s\n",
            "\n",
            "417:\tlearn: 0.0091685\ttotal: 6.87s\tremaining: 6.89s\n",
            "\n",
            "418:\tlearn: 0.0091053\ttotal: 6.89s\tremaining: 6.87s\n",
            "\n",
            "419:\tlearn: 0.0090399\ttotal: 6.91s\tremaining: 6.86s\n",
            "\n",
            "420:\tlearn: 0.0090027\ttotal: 6.92s\tremaining: 6.84s\n",
            "\n",
            "421:\tlearn: 0.0089489\ttotal: 6.94s\tremaining: 6.83s\n",
            "\n",
            "422:\tlearn: 0.0089489\ttotal: 6.96s\tremaining: 6.81s\n",
            "\n",
            "423:\tlearn: 0.0088879\ttotal: 6.97s\tremaining: 6.79s\n",
            "\n",
            "424:\tlearn: 0.0088410\ttotal: 6.99s\tremaining: 6.78s\n",
            "\n",
            "425:\tlearn: 0.0087820\ttotal: 7.01s\tremaining: 6.76s\n",
            "\n",
            "426:\tlearn: 0.0087358\ttotal: 7.02s\tremaining: 6.74s\n",
            "\n",
            "427:\tlearn: 0.0087095\ttotal: 7.04s\tremaining: 6.73s\n",
            "\n",
            "428:\tlearn: 0.0086300\ttotal: 7.05s\tremaining: 6.71s\n",
            "\n",
            "429:\tlearn: 0.0085707\ttotal: 7.07s\tremaining: 6.69s\n",
            "\n",
            "430:\tlearn: 0.0085155\ttotal: 7.1s\tremaining: 6.68s\n",
            "\n",
            "431:\tlearn: 0.0084606\ttotal: 7.12s\tremaining: 6.67s\n",
            "\n",
            "432:\tlearn: 0.0084343\ttotal: 7.13s\tremaining: 6.66s\n",
            "\n",
            "433:\tlearn: 0.0084006\ttotal: 7.15s\tremaining: 6.64s\n",
            "\n",
            "434:\tlearn: 0.0083571\ttotal: 7.17s\tremaining: 6.62s\n",
            "\n",
            "435:\tlearn: 0.0082722\ttotal: 7.18s\tremaining: 6.61s\n",
            "\n",
            "436:\tlearn: 0.0082244\ttotal: 7.2s\tremaining: 6.59s\n",
            "\n",
            "437:\tlearn: 0.0082007\ttotal: 7.22s\tremaining: 6.57s\n",
            "\n",
            "438:\tlearn: 0.0081652\ttotal: 7.23s\tremaining: 6.56s\n",
            "\n",
            "439:\tlearn: 0.0081328\ttotal: 7.25s\tremaining: 6.54s\n",
            "\n",
            "440:\tlearn: 0.0080817\ttotal: 7.26s\tremaining: 6.52s\n",
            "\n",
            "441:\tlearn: 0.0080027\ttotal: 7.29s\tremaining: 6.51s\n",
            "\n",
            "442:\tlearn: 0.0079629\ttotal: 7.3s\tremaining: 6.49s\n",
            "\n",
            "443:\tlearn: 0.0079296\ttotal: 7.32s\tremaining: 6.48s\n",
            "\n",
            "444:\tlearn: 0.0079053\ttotal: 7.33s\tremaining: 6.46s\n",
            "\n",
            "445:\tlearn: 0.0078456\ttotal: 7.35s\tremaining: 6.45s\n",
            "\n",
            "446:\tlearn: 0.0078010\ttotal: 7.37s\tremaining: 6.43s\n",
            "\n",
            "447:\tlearn: 0.0077448\ttotal: 7.39s\tremaining: 6.41s\n",
            "\n",
            "448:\tlearn: 0.0077094\ttotal: 7.41s\tremaining: 6.4s\n",
            "\n",
            "449:\tlearn: 0.0076765\ttotal: 7.43s\tremaining: 6.39s\n",
            "\n",
            "450:\tlearn: 0.0076162\ttotal: 7.44s\tremaining: 6.37s\n",
            "\n",
            "451:\tlearn: 0.0075470\ttotal: 7.46s\tremaining: 6.35s\n",
            "\n",
            "452:\tlearn: 0.0075097\ttotal: 7.48s\tremaining: 6.34s\n",
            "\n",
            "453:\tlearn: 0.0074798\ttotal: 7.49s\tremaining: 6.32s\n",
            "\n",
            "454:\tlearn: 0.0074203\ttotal: 7.51s\tremaining: 6.3s\n",
            "\n",
            "455:\tlearn: 0.0073947\ttotal: 7.53s\tremaining: 6.29s\n",
            "\n",
            "456:\tlearn: 0.0073498\ttotal: 7.54s\tremaining: 6.27s\n",
            "\n",
            "457:\tlearn: 0.0072965\ttotal: 7.56s\tremaining: 6.25s\n",
            "\n",
            "458:\tlearn: 0.0072538\ttotal: 7.57s\tremaining: 6.24s\n",
            "\n",
            "459:\tlearn: 0.0072020\ttotal: 7.59s\tremaining: 6.22s\n",
            "\n",
            "460:\tlearn: 0.0071628\ttotal: 7.61s\tremaining: 6.21s\n",
            "\n",
            "461:\tlearn: 0.0071403\ttotal: 7.63s\tremaining: 6.19s\n",
            "\n",
            "462:\tlearn: 0.0070846\ttotal: 7.64s\tremaining: 6.17s\n",
            "\n",
            "463:\tlearn: 0.0070846\ttotal: 7.66s\tremaining: 6.16s\n",
            "\n",
            "464:\tlearn: 0.0070497\ttotal: 7.67s\tremaining: 6.14s\n",
            "\n",
            "465:\tlearn: 0.0070150\ttotal: 7.69s\tremaining: 6.12s\n",
            "\n",
            "466:\tlearn: 0.0069747\ttotal: 7.7s\tremaining: 6.1s\n",
            "\n",
            "467:\tlearn: 0.0069354\ttotal: 7.73s\tremaining: 6.09s\n",
            "\n",
            "468:\tlearn: 0.0069081\ttotal: 7.75s\tremaining: 6.08s\n",
            "\n",
            "469:\tlearn: 0.0068755\ttotal: 7.76s\tremaining: 6.06s\n",
            "\n",
            "470:\tlearn: 0.0068270\ttotal: 7.77s\tremaining: 6.04s\n",
            "\n",
            "471:\tlearn: 0.0068270\ttotal: 7.79s\tremaining: 6.02s\n",
            "\n",
            "472:\tlearn: 0.0067865\ttotal: 7.8s\tremaining: 6s\n",
            "\n",
            "473:\tlearn: 0.0067864\ttotal: 7.82s\tremaining: 5.99s\n",
            "\n",
            "474:\tlearn: 0.0067378\ttotal: 7.83s\tremaining: 5.97s\n",
            "\n",
            "475:\tlearn: 0.0066935\ttotal: 7.85s\tremaining: 5.95s\n",
            "\n",
            "476:\tlearn: 0.0066716\ttotal: 7.87s\tremaining: 5.94s\n",
            "\n",
            "477:\tlearn: 0.0066405\ttotal: 7.88s\tremaining: 5.92s\n",
            "\n",
            "478:\tlearn: 0.0066143\ttotal: 7.9s\tremaining: 5.9s\n",
            "\n",
            "479:\tlearn: 0.0065741\ttotal: 7.91s\tremaining: 5.89s\n",
            "\n",
            "480:\tlearn: 0.0065364\ttotal: 7.93s\tremaining: 5.87s\n",
            "\n",
            "481:\tlearn: 0.0064992\ttotal: 7.95s\tremaining: 5.85s\n",
            "\n",
            "482:\tlearn: 0.0064547\ttotal: 7.96s\tremaining: 5.84s\n",
            "\n",
            "483:\tlearn: 0.0064546\ttotal: 7.98s\tremaining: 5.82s\n",
            "\n",
            "484:\tlearn: 0.0064161\ttotal: 8s\tremaining: 5.8s\n",
            "\n",
            "485:\tlearn: 0.0063617\ttotal: 8.01s\tremaining: 5.79s\n",
            "\n",
            "486:\tlearn: 0.0063212\ttotal: 8.03s\tremaining: 5.77s\n",
            "\n",
            "487:\tlearn: 0.0063211\ttotal: 8.04s\tremaining: 5.75s\n",
            "\n",
            "488:\tlearn: 0.0063211\ttotal: 8.06s\tremaining: 5.73s\n",
            "\n",
            "489:\tlearn: 0.0063210\ttotal: 8.07s\tremaining: 5.72s\n",
            "\n",
            "490:\tlearn: 0.0063210\ttotal: 8.09s\tremaining: 5.7s\n",
            "\n",
            "491:\tlearn: 0.0062919\ttotal: 8.11s\tremaining: 5.68s\n",
            "\n",
            "492:\tlearn: 0.0062660\ttotal: 8.13s\tremaining: 5.67s\n",
            "\n",
            "493:\tlearn: 0.0062075\ttotal: 8.14s\tremaining: 5.65s\n",
            "\n",
            "494:\tlearn: 0.0061795\ttotal: 8.16s\tremaining: 5.64s\n",
            "\n",
            "495:\tlearn: 0.0061372\ttotal: 8.18s\tremaining: 5.63s\n",
            "\n",
            "496:\tlearn: 0.0061315\ttotal: 8.2s\tremaining: 5.61s\n",
            "\n",
            "497:\tlearn: 0.0061315\ttotal: 8.21s\tremaining: 5.59s\n",
            "\n",
            "498:\tlearn: 0.0060952\ttotal: 8.23s\tremaining: 5.57s\n",
            "\n",
            "499:\tlearn: 0.0060662\ttotal: 8.24s\tremaining: 5.56s\n",
            "\n",
            "500:\tlearn: 0.0060310\ttotal: 8.26s\tremaining: 5.54s\n",
            "\n",
            "501:\tlearn: 0.0060310\ttotal: 8.27s\tremaining: 5.52s\n",
            "\n",
            "502:\tlearn: 0.0060174\ttotal: 8.29s\tremaining: 5.5s\n",
            "\n",
            "503:\tlearn: 0.0059768\ttotal: 8.3s\tremaining: 5.49s\n",
            "\n",
            "504:\tlearn: 0.0059591\ttotal: 8.32s\tremaining: 5.47s\n",
            "\n",
            "505:\tlearn: 0.0059383\ttotal: 8.34s\tremaining: 5.45s\n",
            "\n",
            "506:\tlearn: 0.0059383\ttotal: 8.35s\tremaining: 5.44s\n",
            "\n",
            "507:\tlearn: 0.0059123\ttotal: 8.37s\tremaining: 5.42s\n",
            "\n",
            "508:\tlearn: 0.0058765\ttotal: 8.38s\tremaining: 5.4s\n",
            "\n",
            "509:\tlearn: 0.0058376\ttotal: 8.4s\tremaining: 5.39s\n",
            "\n",
            "510:\tlearn: 0.0057935\ttotal: 8.42s\tremaining: 5.37s\n",
            "\n",
            "511:\tlearn: 0.0057578\ttotal: 8.43s\tremaining: 5.35s\n",
            "\n",
            "512:\tlearn: 0.0057127\ttotal: 8.45s\tremaining: 5.34s\n",
            "\n",
            "513:\tlearn: 0.0056883\ttotal: 8.47s\tremaining: 5.32s\n",
            "\n",
            "514:\tlearn: 0.0056571\ttotal: 8.48s\tremaining: 5.3s\n",
            "\n",
            "515:\tlearn: 0.0056570\ttotal: 8.5s\tremaining: 5.29s\n",
            "\n",
            "516:\tlearn: 0.0056255\ttotal: 8.52s\tremaining: 5.27s\n",
            "\n",
            "517:\tlearn: 0.0056255\ttotal: 8.54s\tremaining: 5.26s\n",
            "\n",
            "518:\tlearn: 0.0055904\ttotal: 8.55s\tremaining: 5.24s\n",
            "\n",
            "519:\tlearn: 0.0055904\ttotal: 8.57s\tremaining: 5.22s\n",
            "\n",
            "520:\tlearn: 0.0055590\ttotal: 8.58s\tremaining: 5.21s\n",
            "\n",
            "521:\tlearn: 0.0055590\ttotal: 8.6s\tremaining: 5.19s\n",
            "\n",
            "522:\tlearn: 0.0055359\ttotal: 8.62s\tremaining: 5.17s\n",
            "\n",
            "523:\tlearn: 0.0055070\ttotal: 8.63s\tremaining: 5.16s\n",
            "\n",
            "524:\tlearn: 0.0054866\ttotal: 8.65s\tremaining: 5.14s\n",
            "\n",
            "525:\tlearn: 0.0054682\ttotal: 8.66s\tremaining: 5.12s\n",
            "\n",
            "526:\tlearn: 0.0054391\ttotal: 8.68s\tremaining: 5.11s\n",
            "\n",
            "527:\tlearn: 0.0054391\ttotal: 8.7s\tremaining: 5.09s\n",
            "\n",
            "528:\tlearn: 0.0053948\ttotal: 8.71s\tremaining: 5.07s\n",
            "\n",
            "529:\tlearn: 0.0053948\ttotal: 8.73s\tremaining: 5.06s\n",
            "\n",
            "530:\tlearn: 0.0053947\ttotal: 8.74s\tremaining: 5.04s\n",
            "\n",
            "531:\tlearn: 0.0053695\ttotal: 8.76s\tremaining: 5.02s\n",
            "\n",
            "532:\tlearn: 0.0053328\ttotal: 8.78s\tremaining: 5.01s\n",
            "\n",
            "533:\tlearn: 0.0053328\ttotal: 8.79s\tremaining: 4.99s\n",
            "\n",
            "534:\tlearn: 0.0052975\ttotal: 8.81s\tremaining: 4.97s\n",
            "\n",
            "535:\tlearn: 0.0052688\ttotal: 8.83s\tremaining: 4.96s\n",
            "\n",
            "536:\tlearn: 0.0052457\ttotal: 8.85s\tremaining: 4.94s\n",
            "\n",
            "537:\tlearn: 0.0052192\ttotal: 8.87s\tremaining: 4.93s\n",
            "\n",
            "538:\tlearn: 0.0051885\ttotal: 8.88s\tremaining: 4.91s\n",
            "\n",
            "539:\tlearn: 0.0051884\ttotal: 8.9s\tremaining: 4.89s\n",
            "\n",
            "540:\tlearn: 0.0051651\ttotal: 8.92s\tremaining: 4.88s\n",
            "\n",
            "541:\tlearn: 0.0051651\ttotal: 8.93s\tremaining: 4.86s\n",
            "\n",
            "542:\tlearn: 0.0051649\ttotal: 8.96s\tremaining: 4.85s\n",
            "\n",
            "543:\tlearn: 0.0051649\ttotal: 8.97s\tremaining: 4.83s\n",
            "\n",
            "544:\tlearn: 0.0051308\ttotal: 8.99s\tremaining: 4.82s\n",
            "\n",
            "545:\tlearn: 0.0051308\ttotal: 9.01s\tremaining: 4.8s\n",
            "\n",
            "546:\tlearn: 0.0051099\ttotal: 9.02s\tremaining: 4.78s\n",
            "\n",
            "547:\tlearn: 0.0051099\ttotal: 9.04s\tremaining: 4.76s\n",
            "\n",
            "548:\tlearn: 0.0051099\ttotal: 9.05s\tremaining: 4.75s\n",
            "\n",
            "549:\tlearn: 0.0051099\ttotal: 9.07s\tremaining: 4.73s\n",
            "\n",
            "550:\tlearn: 0.0050770\ttotal: 9.09s\tremaining: 4.72s\n",
            "\n",
            "551:\tlearn: 0.0050555\ttotal: 9.11s\tremaining: 4.7s\n",
            "\n",
            "552:\tlearn: 0.0050223\ttotal: 9.12s\tremaining: 4.68s\n",
            "\n",
            "553:\tlearn: 0.0050223\ttotal: 9.14s\tremaining: 4.67s\n",
            "\n",
            "554:\tlearn: 0.0050222\ttotal: 9.15s\tremaining: 4.65s\n",
            "\n",
            "555:\tlearn: 0.0050052\ttotal: 9.17s\tremaining: 4.63s\n",
            "\n",
            "556:\tlearn: 0.0050051\ttotal: 9.18s\tremaining: 4.62s\n",
            "\n",
            "557:\tlearn: 0.0050050\ttotal: 9.2s\tremaining: 4.6s\n",
            "\n",
            "558:\tlearn: 0.0049801\ttotal: 9.21s\tremaining: 4.58s\n",
            "\n",
            "559:\tlearn: 0.0049495\ttotal: 9.23s\tremaining: 4.57s\n",
            "\n",
            "560:\tlearn: 0.0049494\ttotal: 9.25s\tremaining: 4.55s\n",
            "\n",
            "561:\tlearn: 0.0049211\ttotal: 9.27s\tremaining: 4.53s\n",
            "\n",
            "562:\tlearn: 0.0048867\ttotal: 9.28s\tremaining: 4.52s\n",
            "\n",
            "563:\tlearn: 0.0048866\ttotal: 9.3s\tremaining: 4.5s\n",
            "\n",
            "564:\tlearn: 0.0048866\ttotal: 9.32s\tremaining: 4.49s\n",
            "\n",
            "565:\tlearn: 0.0048865\ttotal: 9.33s\tremaining: 4.47s\n",
            "\n",
            "566:\tlearn: 0.0048865\ttotal: 9.35s\tremaining: 4.45s\n",
            "\n",
            "567:\tlearn: 0.0048865\ttotal: 9.37s\tremaining: 4.44s\n",
            "\n",
            "568:\tlearn: 0.0048623\ttotal: 9.38s\tremaining: 4.42s\n",
            "\n",
            "569:\tlearn: 0.0048623\ttotal: 9.4s\tremaining: 4.4s\n",
            "\n",
            "570:\tlearn: 0.0048623\ttotal: 9.41s\tremaining: 4.38s\n",
            "\n",
            "571:\tlearn: 0.0048371\ttotal: 9.43s\tremaining: 4.37s\n",
            "\n",
            "572:\tlearn: 0.0048371\ttotal: 9.45s\tremaining: 4.35s\n",
            "\n",
            "573:\tlearn: 0.0048370\ttotal: 9.46s\tremaining: 4.33s\n",
            "\n",
            "574:\tlearn: 0.0048370\ttotal: 9.48s\tremaining: 4.32s\n",
            "\n",
            "575:\tlearn: 0.0048369\ttotal: 9.49s\tremaining: 4.3s\n",
            "\n",
            "576:\tlearn: 0.0048277\ttotal: 9.51s\tremaining: 4.29s\n",
            "\n",
            "577:\tlearn: 0.0048034\ttotal: 9.53s\tremaining: 4.27s\n",
            "\n",
            "578:\tlearn: 0.0047753\ttotal: 9.54s\tremaining: 4.25s\n",
            "\n",
            "579:\tlearn: 0.0047556\ttotal: 9.56s\tremaining: 4.24s\n",
            "\n",
            "580:\tlearn: 0.0047382\ttotal: 9.57s\tremaining: 4.22s\n",
            "\n",
            "581:\tlearn: 0.0047381\ttotal: 9.59s\tremaining: 4.2s\n",
            "\n",
            "582:\tlearn: 0.0047381\ttotal: 9.61s\tremaining: 4.18s\n",
            "\n",
            "583:\tlearn: 0.0047380\ttotal: 9.62s\tremaining: 4.17s\n",
            "\n",
            "584:\tlearn: 0.0047380\ttotal: 9.64s\tremaining: 4.15s\n",
            "\n",
            "585:\tlearn: 0.0047380\ttotal: 9.65s\tremaining: 4.13s\n",
            "\n",
            "586:\tlearn: 0.0047378\ttotal: 9.67s\tremaining: 4.12s\n",
            "\n",
            "587:\tlearn: 0.0047377\ttotal: 9.69s\tremaining: 4.1s\n",
            "\n",
            "588:\tlearn: 0.0047149\ttotal: 9.7s\tremaining: 4.08s\n",
            "\n",
            "589:\tlearn: 0.0047045\ttotal: 9.72s\tremaining: 4.07s\n",
            "\n",
            "590:\tlearn: 0.0047045\ttotal: 9.74s\tremaining: 4.05s\n",
            "\n",
            "591:\tlearn: 0.0047045\ttotal: 9.75s\tremaining: 4.04s\n",
            "\n",
            "592:\tlearn: 0.0047044\ttotal: 9.77s\tremaining: 4.02s\n",
            "\n",
            "593:\tlearn: 0.0047044\ttotal: 9.78s\tremaining: 4s\n",
            "\n",
            "594:\tlearn: 0.0047043\ttotal: 9.79s\tremaining: 3.98s\n",
            "\n",
            "595:\tlearn: 0.0047043\ttotal: 9.81s\tremaining: 3.97s\n",
            "\n",
            "596:\tlearn: 0.0047042\ttotal: 9.82s\tremaining: 3.95s\n",
            "\n",
            "597:\tlearn: 0.0047042\ttotal: 9.84s\tremaining: 3.93s\n",
            "\n",
            "598:\tlearn: 0.0047042\ttotal: 9.85s\tremaining: 3.92s\n",
            "\n",
            "599:\tlearn: 0.0046815\ttotal: 9.87s\tremaining: 3.9s\n",
            "\n",
            "600:\tlearn: 0.0046642\ttotal: 9.89s\tremaining: 3.88s\n",
            "\n",
            "601:\tlearn: 0.0046642\ttotal: 9.9s\tremaining: 3.87s\n",
            "\n",
            "602:\tlearn: 0.0046642\ttotal: 9.92s\tremaining: 3.85s\n",
            "\n",
            "603:\tlearn: 0.0046641\ttotal: 9.94s\tremaining: 3.83s\n",
            "\n",
            "604:\tlearn: 0.0046530\ttotal: 9.95s\tremaining: 3.81s\n",
            "\n",
            "605:\tlearn: 0.0046399\ttotal: 9.96s\tremaining: 3.8s\n",
            "\n",
            "606:\tlearn: 0.0046116\ttotal: 9.98s\tremaining: 3.78s\n",
            "\n",
            "607:\tlearn: 0.0045869\ttotal: 9.99s\tremaining: 3.76s\n",
            "\n",
            "608:\tlearn: 0.0045618\ttotal: 10s\tremaining: 3.75s\n",
            "\n",
            "609:\tlearn: 0.0045618\ttotal: 10s\tremaining: 3.73s\n",
            "\n",
            "610:\tlearn: 0.0045617\ttotal: 10s\tremaining: 3.71s\n",
            "\n",
            "611:\tlearn: 0.0045617\ttotal: 10.1s\tremaining: 3.7s\n",
            "\n",
            "612:\tlearn: 0.0045616\ttotal: 10.1s\tremaining: 3.68s\n",
            "\n",
            "613:\tlearn: 0.0045616\ttotal: 10.1s\tremaining: 3.66s\n",
            "\n",
            "614:\tlearn: 0.0045616\ttotal: 10.1s\tremaining: 3.65s\n",
            "\n",
            "615:\tlearn: 0.0045616\ttotal: 10.1s\tremaining: 3.63s\n",
            "\n",
            "616:\tlearn: 0.0045615\ttotal: 10.1s\tremaining: 3.61s\n",
            "\n",
            "617:\tlearn: 0.0045615\ttotal: 10.2s\tremaining: 3.6s\n",
            "\n",
            "618:\tlearn: 0.0045615\ttotal: 10.2s\tremaining: 3.58s\n",
            "\n",
            "619:\tlearn: 0.0045615\ttotal: 10.2s\tremaining: 3.56s\n",
            "\n",
            "620:\tlearn: 0.0045615\ttotal: 10.2s\tremaining: 3.55s\n",
            "\n",
            "621:\tlearn: 0.0045496\ttotal: 10.2s\tremaining: 3.53s\n",
            "\n",
            "622:\tlearn: 0.0045496\ttotal: 10.2s\tremaining: 3.52s\n",
            "\n",
            "623:\tlearn: 0.0045495\ttotal: 10.2s\tremaining: 3.5s\n",
            "\n",
            "624:\tlearn: 0.0045495\ttotal: 10.3s\tremaining: 3.48s\n",
            "\n",
            "625:\tlearn: 0.0045495\ttotal: 10.3s\tremaining: 3.46s\n",
            "\n",
            "626:\tlearn: 0.0045494\ttotal: 10.3s\tremaining: 3.45s\n",
            "\n",
            "627:\tlearn: 0.0045494\ttotal: 10.3s\tremaining: 3.43s\n",
            "\n",
            "628:\tlearn: 0.0045494\ttotal: 10.3s\tremaining: 3.41s\n",
            "\n",
            "629:\tlearn: 0.0045493\ttotal: 10.3s\tremaining: 3.4s\n",
            "\n",
            "630:\tlearn: 0.0045493\ttotal: 10.4s\tremaining: 3.38s\n",
            "\n",
            "631:\tlearn: 0.0045492\ttotal: 10.4s\tremaining: 3.36s\n",
            "\n",
            "632:\tlearn: 0.0045492\ttotal: 10.4s\tremaining: 3.35s\n",
            "\n",
            "633:\tlearn: 0.0045491\ttotal: 10.4s\tremaining: 3.33s\n",
            "\n",
            "634:\tlearn: 0.0045491\ttotal: 10.4s\tremaining: 3.31s\n",
            "\n",
            "635:\tlearn: 0.0045272\ttotal: 10.4s\tremaining: 3.3s\n",
            "\n",
            "636:\tlearn: 0.0044891\ttotal: 10.5s\tremaining: 3.28s\n",
            "\n",
            "637:\tlearn: 0.0044654\ttotal: 10.5s\tremaining: 3.27s\n",
            "\n",
            "638:\tlearn: 0.0044457\ttotal: 10.5s\tremaining: 3.25s\n",
            "\n",
            "639:\tlearn: 0.0044457\ttotal: 10.5s\tremaining: 3.24s\n",
            "\n",
            "640:\tlearn: 0.0044457\ttotal: 10.5s\tremaining: 3.22s\n",
            "\n",
            "641:\tlearn: 0.0044457\ttotal: 10.6s\tremaining: 3.2s\n",
            "\n",
            "642:\tlearn: 0.0044456\ttotal: 10.6s\tremaining: 3.19s\n",
            "\n",
            "643:\tlearn: 0.0044269\ttotal: 10.6s\tremaining: 3.17s\n",
            "\n",
            "644:\tlearn: 0.0044267\ttotal: 10.6s\tremaining: 3.15s\n",
            "\n",
            "645:\tlearn: 0.0044267\ttotal: 10.6s\tremaining: 3.14s\n",
            "\n",
            "646:\tlearn: 0.0044267\ttotal: 10.6s\tremaining: 3.12s\n",
            "\n",
            "647:\tlearn: 0.0044267\ttotal: 10.6s\tremaining: 3.1s\n",
            "\n",
            "648:\tlearn: 0.0044266\ttotal: 10.7s\tremaining: 3.09s\n",
            "\n",
            "649:\tlearn: 0.0044266\ttotal: 10.7s\tremaining: 3.07s\n",
            "\n",
            "650:\tlearn: 0.0044266\ttotal: 10.7s\tremaining: 3.06s\n",
            "\n",
            "651:\tlearn: 0.0044266\ttotal: 10.7s\tremaining: 3.04s\n",
            "\n",
            "652:\tlearn: 0.0044266\ttotal: 10.7s\tremaining: 3.02s\n",
            "\n",
            "653:\tlearn: 0.0044266\ttotal: 10.7s\tremaining: 3.01s\n",
            "\n",
            "654:\tlearn: 0.0044265\ttotal: 10.8s\tremaining: 2.99s\n",
            "\n",
            "655:\tlearn: 0.0044265\ttotal: 10.8s\tremaining: 2.97s\n",
            "\n",
            "656:\tlearn: 0.0044265\ttotal: 10.8s\tremaining: 2.96s\n",
            "\n",
            "657:\tlearn: 0.0044130\ttotal: 10.8s\tremaining: 2.94s\n",
            "\n",
            "658:\tlearn: 0.0043872\ttotal: 10.8s\tremaining: 2.93s\n",
            "\n",
            "659:\tlearn: 0.0043682\ttotal: 10.8s\tremaining: 2.91s\n",
            "\n",
            "660:\tlearn: 0.0043682\ttotal: 10.9s\tremaining: 2.89s\n",
            "\n",
            "661:\tlearn: 0.0043682\ttotal: 10.9s\tremaining: 2.88s\n",
            "\n",
            "662:\tlearn: 0.0043682\ttotal: 10.9s\tremaining: 2.86s\n",
            "\n",
            "663:\tlearn: 0.0043682\ttotal: 10.9s\tremaining: 2.84s\n",
            "\n",
            "664:\tlearn: 0.0043681\ttotal: 10.9s\tremaining: 2.83s\n",
            "\n",
            "665:\tlearn: 0.0043681\ttotal: 10.9s\tremaining: 2.81s\n",
            "\n",
            "666:\tlearn: 0.0043681\ttotal: 11s\tremaining: 2.79s\n",
            "\n",
            " 60%|██████    | 60/100 [45:53<13:35, 20.38s/it, best loss: -0.8295819935691319]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEGIkF9lkoTn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "889c2895-da2b-450d-ddb2-04bf2140a5e0"
      },
      "source": [
        "best_params"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'border_count': 113,\n",
              " 'depth': 5,\n",
              " 'grow_policy': 1,\n",
              " 'iterations': 2,\n",
              " 'learning_rate': 0.6346362946956222}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TErYxk88mmX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4ff8fc50-9cb3-4ae6-d5de-a487e4d9d77a"
      },
      "source": [
        "best_params[ 'grow_policy' ] = policy_list[ best_params[ 'grow_policy' ] ] #best_params[ 'grow_policy' ] holds order of opt value, we extract real val from list\n",
        "best_params[ 'depth' ] = depth_list[ best_params[ 'depth' ] ] #same here\n",
        "best_params[ 'border_count' ] = border_list[ best_params[ 'border_count' ] ] #same here\n",
        "best_params[ 'iterations' ] = iter_list[ best_params[ 'iterations' ] ] #same here\n",
        "print(best_params)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'border_count': 114, 'depth': 9, 'grow_policy': 'Depthwise', 'iterations': 12, 'learning_rate': 0.6346362946956222}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SEKFDVi5jN0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rosemary.best_params = best_params"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXaW0tptVxoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Displays available classifiers\n",
        "#rosemary.print_class_options()\n",
        "\n",
        "#You can choose classifier by specifying first few letter\n",
        "rosemary.choose_classifier( 'Cat', backtest = True )\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6uZMztgKAk1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "29ba5112-91ad-429e-884e-fbc60db0c271"
      },
      "source": [
        "cash, cash_best_possible = rosemary.train_tuned()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " PREPROCESSING \n",
            "\n",
            "\n",
            " DEFINING CLASSES \n",
            "\n",
            "0:\tlearn: 0.4099345\ttotal: 543ms\tremaining: 5.97s\n",
            "1:\tlearn: 0.2971299\ttotal: 1.06s\tremaining: 5.29s\n",
            "2:\tlearn: 0.1989801\ttotal: 1.67s\tremaining: 5s\n",
            "3:\tlearn: 0.1561008\ttotal: 2.18s\tremaining: 4.37s\n",
            "4:\tlearn: 0.1226331\ttotal: 2.7s\tremaining: 3.78s\n",
            "5:\tlearn: 0.1047981\ttotal: 3.14s\tremaining: 3.14s\n",
            "6:\tlearn: 0.0905736\ttotal: 3.7s\tremaining: 2.64s\n",
            "7:\tlearn: 0.0765475\ttotal: 4.22s\tremaining: 2.11s\n",
            "8:\tlearn: 0.0602804\ttotal: 4.77s\tremaining: 1.59s\n",
            "9:\tlearn: 0.0481744\ttotal: 5.29s\tremaining: 1.06s\n",
            "10:\tlearn: 0.0430476\ttotal: 5.72s\tremaining: 520ms\n",
            "11:\tlearn: 0.0363046\ttotal: 6.26s\tremaining: 0us\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5c964MEYBZE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "9028b942-3d8f-4801-8372-e66316999941"
      },
      "source": [
        "plt.plot(cash)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe22748be10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B43GU48DTYBe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "e986a5d7-7825-44f2-bdf8-828eb17c8bee"
      },
      "source": [
        "plt.plot(cash_best_possible)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe22766cba8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xhlyQT3bahU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK7oXTZepUTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJYGU8EzY9tJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plot confusion matrix\n",
        "rosemary.confusion_matrix()\n",
        "\n",
        "#if it does not work, try the code below. \n",
        "#You need to have test_pred first\n",
        "#cm = np.array(confusion_matrix( rosemary.y_test, test_pred ))\n",
        "#plot_confusion_matrix( cm = cm, target_names = [ 'nothing', 'spike' ] )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giHasdJMY9rp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get back-test results\n",
        "pred_returns, returns_with_grand_truth = rosemary.train_tuned()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukxnTC4OaMHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot( pred_returns )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMuWJEOEY9pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eclLQCu4rvOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W_bOrWYsPK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u0H-f3krlv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cCaMnyGXdds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJXDgjw1Y9no",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjyxXMp_Y9mR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "global X\n",
        "global Y\n",
        "\n",
        "X = rosemary.X_train\n",
        "Y = rosemary.y_train\n"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfdIR7oPLbfe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "07dd6975-209b-458e-dc66-068717c48d62"
      },
      "source": [
        "sample( SPACE )"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'border_count': 112.08976256884324,\n",
              " 'depth': 5,\n",
              " 'grow_policy': 'Lossguide',\n",
              " 'iterations': 996.7360741510896,\n",
              " 'learning_rate': 0.018859030663855587}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LTFCWGeCWZe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XWVswXiCc3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5q7d3bjSOVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1fb0a821-2df3-4ef4-b481-c0ab9dca7f07"
      },
      "source": [
        "print(best)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'border_count': 121.62068796436915, 'depth': 6, 'feature_fraction': 1, 'iterations': 14.456372424780248, 'learning_rate': 0.5409026700346178}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-ODX4RZCfNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMtzD9BfRXhJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trials.trials.results[0]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}