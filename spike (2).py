# -*- coding: utf-8 -*-
"""Spike.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_RjJej0wk6hOcjON4QcknRZz67OUM8VA
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install yahoofinancials

!pip install pandas-ta

!pip install pycaret

!pip install costcla

!pip install arch

import pandas as pd
from numpy import mean
from datetime import datetime
import matplotlib.pyplot as plt
from yahoofinancials import YahooFinancials
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import arch
from sklearn.metrics import f1_score
import numpy as np
import seaborn as sns
import scipy.stats as st
from sklearn.model_selection import cross_val_score
from imblearn.over_sampling import ADASYN
from imblearn.combine import SMOTETomek
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import VarianceThreshold
from sklearn.ensemble import ExtraTreesClassifier,RandomForestClassifier
from catboost import CatBoostClassifier
from catboost.utils import Pool, get_confusion_matrix
from lightgbm import LGBMClassifier
from sklearn.svm import SVC
from collections import Counter
from sklearn.metrics import brier_score_loss
from sklearn.neural_network.multilayer_perceptron import MLPClassifier
from mlxtend.classifier import StackingCVClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble.weight_boosting import AdaBoostClassifier
from sklearn.linear_model import RidgeClassifierCV
from sklearn.svm import LinearSVC, NuSVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import StackingClassifier
from sklearn.calibration import CalibratedClassifierCV
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from costcla.metrics import cost_loss, savings_score
from costcla.models import CostSensitiveRandomForestClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import KFold
from sklearn.metrics import make_scorer
from sklearn.metrics import f1_score, precision_score, precision_recall_curve,recall_score, accuracy_score
from sklearn.metrics import plot_confusion_matrix
from yellowbrick.classifier import ConfusionMatrix
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from statsmodels.stats.outliers_influence import summary_table
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import KFold
from xgboost import XGBClassifier, plot_importance, DMatrix, plot_tree
import pandas_ta as ta
import warnings

warnings.simplefilter('ignore')

ticker_details = pd.read_excel('Ticker List.xlsx')

ticker = ticker_details['Ticker'].to_list()
names = ticker_details['Description'].to_list()
print(names)



#Extracting Data from Yahoo Finance and Adding them to Values table using date as key
end_date= "2020-06-19"
start_date = "2000-01-01"
date_range = pd.bdate_range(start=start_date,end=end_date)
values = pd.DataFrame({ 'Date': date_range})
values['Date']= pd.to_datetime(values['Date'])

#Extracting Data from Yahoo Finance and Adding them to Values table using date as key
for i in ticker:
    raw_data = YahooFinancials(i)
    raw_data = raw_data.get_historical_price_data(start_date, end_date, "daily")
    df = pd.DataFrame(raw_data[i]['prices'])[['formatted_date','adjclose']]
    df.columns = ['Date1',i]
    df['Date1']= pd.to_datetime(df['Date1'])
    values = values.merge(df,how='left',left_on='Date',right_on='Date1')
    values = values.drop(labels='Date1', axis=1)

#Renaming columns to represent instrument names rather than their ticker codes for ease of readability
names.insert(0,'Date')
values.columns = names
#print(values.shape)
#print(values.isna().sum())
values.head()

#Front filling the NaN values in the data set
values = values.fillna(method="ffill",axis=0)
values = values.fillna(method="bfill",axis=0)
values.isna().sum()

# Co-ercing numeric type to all columns except Date
cols=values.columns.drop('Date')
values[cols] = values[cols].apply(pd.to_numeric,errors='coerce').round(decimals=4)
#print(values.tail())

imp = ['Gold','USD Index', 'Oil', 'SPX','VIX', 'High Yield Fund' , 'Nikkei', 'Dax', '10Yr', '2Yr' , 'EEM' ,'XLE', 'XLF', 'XLI', 'AUDJPY', 'XLK', 'SSE', 'XLP','XLY', 'XLU', 'XLV', 'Lockheed', 'Lumber', 'Copper']
# Calculating Short term -Historical Returns
change_days = [1,3,5,14,21]

data = pd.DataFrame(data=values['Date'])
for i in change_days:
    print(data.shape)
    x= values[cols].pct_change(periods=i).add_suffix("-T-"+str(i))
    data=pd.concat(objs=(data,x),axis=1)
    x=[]
#print(data.shape)

#Calculating volatilities for SPX
volatility = pd.DataFrame(values['Date'],columns=['Date'])
volatility['Date']=pd.to_datetime(volatility['Date'],format='%Y-%b-%d')
volatility['SPX/vol5']=values['SPX'].rolling(window=5).std()-1
volatility['SPX/vol15']=values['SPX'].rolling(window=15).std()-1
volatility['SPX/vol30']=values['SPX'].rolling(window=30).std()-1
volatility['SPX/vol60']=values['SPX'].rolling(window=60).std()-1
volatility['SPX/vol90']=values['SPX'].rolling(window=90).std()-1
volatility['SPX/vol180']=values['SPX'].rolling(window=180).std()-1
data['Date'] = pd.to_datetime(data['Date'], format='%Y-%b-%d')
data = pd.merge(left=data,right=volatility,how='left',on='Date')
data.isna().sum()

df['log_price'] = np.log(values['SPX'])
df['pct_change'] = df['log_price'].diff()
df.head()

df['stdev14'] = df['pct_change'].rolling(window=14, center=False).std()
df['hvol14'] = df['stdev14'] * (252**0.5)
df = df.dropna()
df.head()

returns = df['pct_change'] * 100
am = arch.arch_model(returns)

res = am.fit(disp='off')
res.summary()

df['forecast_vol'] = 0.1 * np.sqrt(res.params['omega'] + res.params['alpha[1]'] * res.resid**2 + res.conditional_volatility**2 * res.params['beta[1]'])
df.tail()

df.drop(['log_price', 'pct_change', 'stdev14', 'hvol14'], axis = 1)

df['forecast_vol'] = df['forecast_vol'].diff()
df['forecast_vol'].tail()
plt.plot(df['forecast_vol'])

from numpy import cumsum, log, polyfit, sqrt, std, subtract

spx_close = values[['SPX']]
_ = spx_close.plot(figsize = (20,10),
                   linewidth = 3,
                   fontsize = 14)

sns.set();
spx_close = values[['SPX']].copy()
lag1, lag2 = 2, 100
lags = range(lag1, lag2)
tau = [sqrt(std(subtract(spx_close[lag:], spx_close[:-lag]))) for lag in lags]
m = polyfit(log(lags),log(tau),1)
hurst = m[0]*2
print('hurst =', hurst [0])

#Calculating Moving averages for SPX
moving_avg = pd.DataFrame(values['Date'],columns=['Date'])
moving_avg['Date']=pd.to_datetime(moving_avg['Date'],format='%Y-%b-%d')
moving_avg['SPX/15SMA'] = (values['SPX']/(values['SPX'].rolling(window=15).mean()))-1
moving_avg['SPX/30SMA'] = (values['SPX']/(values['SPX'].rolling(window=30).mean()))-1
moving_avg['SPX/60SMA'] = (values['SPX']/(values['SPX'].rolling(window=60).mean()))-1
moving_avg['SPX/90SMA'] = (values['SPX']/(values['SPX'].rolling(window=90).mean()))-1
moving_avg['SPX/180SMA'] = (values['SPX']/(values['SPX'].rolling(window=180).mean()))-1
moving_avg['SPX/90EMA'] = (values['SPX']/(values['SPX'].ewm(span=90,adjust=True,ignore_na=True).mean()))-1
moving_avg['SPX/180EMA'] = (values['SPX']/(values['SPX'].ewm(span=180,adjust=True,ignore_na=True).mean()))-1
moving_avg = moving_avg.dropna(axis=0)
#print(moving_avg.shape)
#print(moving_avg.head())

#Merging Moving Average values to the feature space
#print(data.shape)
data['Date']=pd.to_datetime(data['Date'],format='%Y-%b-%d')
data = pd.merge(left=data,right=moving_avg,how='left',on='Date')
#print(data.shape)
data.isna().sum()

# Calculating Long term Historical Returns
change_days = [60,90,180,250]

for i in change_days:
    print(data.shape)
    x= values[imp].pct_change(periods=i).add_suffix("-T-"+str(i))
    data=pd.concat(objs=(data,x),axis=1)
    x=[]
#print(data.shape)

#Calculating forward returns for Target
y = pd.DataFrame(data=values['Date'])
print(y.shape)

y['SPX-T+14']=values["SPX"].pct_change(periods=-14)
y['SPX-T+22']=values["SPX"].pct_change(periods=-22)
print(y.shape)
y.isna().sum()
len(names)

# Removing NAs
print(data.shape)
data = data[data['SPX-T-250'].notna()]
y = y[y['SPX-T+22'].notna()]
print(data.shape)
print(y.shape)

#Adding Target Variables
data = pd.merge(left=data,right=y,how='inner',on='Date',suffixes=(False,False))
print(data.shape)
data.isna().sum()

sns.distplot(data['SPX-T+14'])
sns.distplot(data['SPX-T+22'])

#Select Threshold p (left tail probability)
p1 = 0.25
p2 = 0.7
#Get z-Value
z1 = st.norm.ppf(p1)
z2 = st.norm.ppf(p2)
print(z1)
print(z2)

#Calculating Threshold (t) for each Y
t_141 = round((z1*np.std(data["SPX-T+14"]))+np.mean(data["SPX-T+14"]),5)
t_142 = round((z2*np.std(data["SPX-T+14"]))+np.mean(data["SPX-T+14"]),5)
t_221 = round((z1*np.std(data["SPX-T+22"]))+np.mean(data["SPX-T+22"]),5)
t_222 = round((z2*np.std(data["SPX-T+22"]))+np.mean(data["SPX-T+22"]),5)

print("t_141=",t_141)
print("t_142=",t_142)
#print("t_221=",t_221)
#print("t_222=",t_222)

#Creating Labels
data['Y-141'] = (data['SPX-T+14']< t_141)*1
data['Y-142'] = (data['SPX-T+14']> t_142)*1
data['Y-221']= (data['SPX-T+22']< t_221)*1
data['Y-222']= (data['SPX-T+22']> t_222)*1
#print("Y-141", sum(data['Y-141']))
#print("Y-142", sum(data['Y-142']))
#print("Y-221", sum(data['Y-221']))
#print("Y-222", sum(data['Y-222']))

data_TEST = data.iloc[: -150]

data = data.drop(['SPX-T+14','SPX-T+22','Date'],axis=1)
#print(data.head())

data_14 = data.drop(['Y-221','Y-222','Y-141'],axis=1)
X = data_14.drop(['Y-142'],axis=1)
y= data_14['Y-142']
feature_names = list(X.columns.values)
#'XLP-T-180','Dax-T-180','XLI-T-3','XLU-T-250','Nikkei-T-180','XLY-T-21','SPX/60SMA','SPX/90EMA','XLI-T-60','SSE-T-180','XLV-T-250','SPX-T-180','Dax-T-90','EEM-T-90','XLY-T-90','SPX/15SMA','High Yield Fund-T-180','SPX-T-90','2Yr-T-90','SPX/180EMA','XLP-T-250','XLE-T-180','XLK-T-180','SPX/180SMA','SPX/90SMA','XLK-T-180','SPX-T-60','SPX-T-21','SPX-T-14','SPX-T-250','SPX-T-5','SPX-T-3','SPX-T-1','XLI-T-90','2Yr-T-180','High Yield Fund-T-250','XLY-T-180','XLI-T-180','SPX/30SMA','XLI-T-250','Dax-T-250','EEM-T-250','XLI-T-14'

#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#

import time
from statsmodels.stats.outliers_influence import variance_inflation_factor    
from joblib import Parallel, delayed

# Defining the function that you will run later
def calculate_vif_(X, thresh=10.0):
    variables = [X.columns[i] for i in range(X.shape[1])]
    dropped=True
    while dropped:
        dropped=False
        print(len(variables))
        vif = Parallel(n_jobs=-1,verbose=5)(delayed(variance_inflation_factor)(X[variables].values, ix) for ix in range(len(variables)))

        maxloc = vif.index(max(vif))
        if max(vif) > thresh:
            print(time.ctime() + ' dropping \'' + X[variables].columns[maxloc] + '\' at index: ' + str(maxloc))
            variables.pop(maxloc)
            dropped=True

    print('Remaining variables:')
    print([variables])
    return X[[i for i in variables]]

X = data_14.drop(['Y-142'],axis=1) # Selecting your data

X2 = calculate_vif_(X,10) # Actually running the function

data_14.drop(data_14.columns.difference(['forecast_vol','USD Index-T-1', 
                                         'VIX-T-1', 'Gold-T-1', 'Oil-T-1',
                                         'Nikkei-T-1', 'Dax-T-1', '10Yr-T-1',
                                         '2Yr-T-1', 'EEM-T-1', 
                                         'High Yield Fund-T-1', 'XLE-T-1',
                                         'XLF-T-1', 'XLI-T-1', 'AUDJPY-T-1',
                                         'XLK-T-1', 'SSE-T-1', 'XLP-T-1',
                                         'XLY-T-1', 'XLU-T-1', 'XLV-T-1',
                                         'Lumber-T-1', 'Lockheed-T-1', 
                                         'Copper-T-1', 'USD Index-T-3',
                                         'VIX-T-3', 'Gold-T-3', 'Oil-T-3',
                                         'Nikkei-T-3', 'Dax-T-3', '10Yr-T-3',
                                         '2Yr-T-3', 'EEM-T-3', 'High Yield Fund-T-3', 
                                         'XLE-T-3', 'XLF-T-3', 'AUDJPY-T-3', 'XLK-T-3', 
                                         'SSE-T-3', 'XLP-T-3', 'XLU-T-3', 'XLV-T-3',
                                         'Lumber-T-3', 'Lockheed-T-3', 'Copper-T-3',
                                         'USD Index-T-5', 'VIX-T-5', 'Gold-T-5', 
                                         'Oil-T-5', 'Nikkei-T-5', 'Dax-T-5', '10Yr-T-5',
                                         '2Yr-T-5', 'EEM-T-5', 'High Yield Fund-T-5', 
                                         'XLE-T-5', 'XLF-T-5', 'AUDJPY-T-5', 'XLK-T-5', 
                                         'SSE-T-5', 'XLP-T-5', 'XLY-T-5', 'XLU-T-5',
                                         'XLV-T-5', 'Lumber-T-5', 'Lockheed-T-5', 
                                         'Copper-T-5', 'USD Index-T-14', 'VIX-T-14',
                                         'Gold-T-14', 'Oil-T-14', 'Nikkei-T-14', 
                                         'Dax-T-14', '10Yr-T-14', '2Yr-T-14', 
                                         'EEM-T-14', 'High Yield Fund-T-14',
                                         'XLE-T-14', 'XLF-T-14', 'XLI-T-14', 
                                         'AUDJPY-T-14', 'XLK-T-14', 'SSE-T-14',
                                         'XLP-T-14', 'XLU-T-14', 'XLV-T-14',
                                         'Lumber-T-14', 'Lockheed-T-14', 
                                         'Copper-T-14', 'USD Index-T-21', 'VIX-T-21',
                                         'Gold-T-21', 'Oil-T-21', 'Nikkei-T-21',
                                         '10Yr-T-21', '2Yr-T-21', 'High Yield Fund-T-21',
                                         'XLE-T-21', 'XLF-T-21', 'AUDJPY-T-21', 'XLK-T-21',
                                         'SSE-T-21', 'XLP-T-21', 'XLU-T-21', 'XLV-T-21',
                                         'Lumber-T-21', 'Lockheed-T-21', 'Copper-T-21', 
                                         'SPX/vol5', 'SPX/vol180', 'Gold-T-60',
                                         'USD Index-T-60', 'Oil-T-60', 'Nikkei-T-60',
                                         'Dax-T-60', '10Yr-T-60', 'EEM-T-60',
                                         'AUDJPY-T-60', 'XLK-T-60', 'SSE-T-60', 
                                         'XLP-T-60', 'XLY-T-60', 'XLU-T-60',
                                         'XLV-T-60', 'Lockheed-T-60', 'Lumber-T-60', 
                                         'Copper-T-60', 'Gold-T-90', 'USD Index-T-90', 
                                         'Oil-T-90', 'High Yield Fund-T-90', '10Yr-T-90',
                                         'XLF-T-90', 'AUDJPY-T-90', 'SSE-T-90', 'XLU-T-90',
                                         'XLV-T-90', 'Lockheed-T-90', 'Lumber-T-90',
                                         'Gold-T-180', 'Oil-T-180', 'VIX-T-180', 
                                         '10Yr-T-180', 'AUDJPY-T-180', 'XLU-T-180', 
                                         'XLV-T-180', 'Lumber-T-180', 'USD Index-T-250',
                                         'Oil-T-250', 'VIX-T-250', 'Nikkei-T-250',
                                         '10Yr-T-250', '2Yr-T-250', 'XLE-T-250', 
                                         'XLF-T-250', 'XLK-T-250', 'SSE-T-250',
                                         'XLY-T-250', 'Lockheed-T-250', 
                                         'Lumber-T-250', 'Copper-T-250']), 1, inplace=True)

print(data_14.columns)

X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size = 0.25, random_state = 42, stratify = y)

mm = MinMaxScaler()
X_train = mm.fit_transform(X_train)
X_test = mm.transform(X_test)

print('y_train class distribution')
print(y_train.value_counts(normalize=True))

print('y_test class distribution')
print(y_test.value_counts(normalize=True))

scorers = {'precision_score': make_scorer(precision_score), 'f1_score': make_scorer(f1_score), 'recall_score': make_scorer(recall_score), 'accuracy_score': make_scorer(accuracy_score)}

lgbm = LGBMClassifier()
knn = KNeighborsClassifier()
catb = CatBoostClassifier()
xgb = XGBClassifier()
et = ExtraTreesClassifier()

#Extratrees Parameters
n_estimators = np.arange(50,350)
max_depth = np.arange(5,350)
max_features = ['sqrt', 'log2']
param_grid = dict(n_estimators = n_estimators, max_features = max_features, max_depth = max_depth)
et = ExtraTreesClassifier()
randomized = RandomizedSearchCV(et, param_grid, scoring = 'f1', cv = 2, n_iter = 10)
randomized.fit(X_train,y_train)
randomized.best_estimator_

#knn Parameters
#k_range = np.arange(1,100)
#weights = ["uniform","distance"]
#algorithm = ["auto", "ball_tree","kd_tree","brute"]
#p = [1,2]
#leaf_size_range = np.arange(1,100)
#param_grid = dict(n_neighbors = k_range, weights = weights, leaf_size = leaf_size_range,p = p)
#knn = KNeighborsClassifier()
#randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 3, n_iter = 20)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#CatBoost Parameters
#iterations = np.arange(1000,1001)
#param_grid = dict(iterations = iterations)
#catb = CatBoostClassifier()
#randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 2, n_iter = 2)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

def rand_search_wrapper(refit_score='f1_score'):
  skf = StratifiedKFold(n_splits=10)
  rand_search = RandomizedSearchCV(et, param_distributions= param_grid, scoring = 'f1', cv = skf, return_train_score= True, n_jobs = -1)
  rand_search.fit(X_train, y_train)
  y_pred = rand_search.predict(X_test)
  print('Best params for {}'.format(refit_score))
  print(rand_search.best_params_)
  print('\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))
  print(pd.DataFrame(confusion_matrix(y_test, y_pred),
               columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))
  return rand_search

rand_search_et = rand_search_wrapper(refit_score='f1_score')

y_scores = rand_search_et.predict_proba(X_test)[:, 1]

p, r, thresholds = precision_recall_curve(y_test, y_scores)

def adjusted_classes(y_scores, t):
    
    return [1 if y >= t else 0 for y in y_scores]

def precision_recall_threshold(t=0.5):
    
    # generate new class predictions based on the adjusted_classes
    # function above and view the resulting confusion matrix.
    y_pred_adj = adjusted_classes(y_scores, t)
    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),
                       columns=['pred_neg', 'pred_pos'], 
                       index=['neg', 'pos']))
    
    # plot the curve
    plt.figure(figsize=(8,8))
    plt.title("Precision and Recall curve ^ = current threshold")
    plt.step(r, p, color='b', alpha=0.2,
             where='post')
    plt.fill_between(r, p, step='post', alpha=0.2,
                     color='b')
    plt.ylim([0.5, 1.01]);
    plt.xlim([0.5, 1.01]);
    plt.xlabel('Recall');
    plt.ylabel('Precision');
    
    # plot the current threshold on the line
    close_default_et = np.argmin(np.abs(thresholds - t))
    plt.plot(r[close_default_et], p[close_default_et], '^', c='k',
            markersize=15)

precision_recall_threshold(0.35)

lgbm = LGBMClassifier()
knn = KNeighborsClassifier()
catb = CatBoostClassifier()
xgb = XGBClassifier()
et = ExtraTreesClassifier()

#Extratrees Parameters
#n_estimators = np.arange(50,350)
#max_depth = np.arange(5,350)
#max_features = ['sqrt', 'log2']
#param_grid = dict(n_estimators = n_estimators, criterion = criterion, max_features = max_features, max_depth = max_depth)
#et = ExtraTreesClassifier()
#randomized = RandomizedSearchCV(et, param_grid, scoring = 'f1', cv = 2, n_iter = 10)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#knn Parameters
k_range = np.arange(1,100)
weights = ["uniform","distance"]
algorithm = ["auto", "ball_tree","kd_tree","brute"]
p = [1,2]
leaf_size_range = np.arange(1,100)
param_grid = dict(n_neighbors = k_range, weights = weights, leaf_size = leaf_size_range,p = p)
knn = KNeighborsClassifier()
randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 3, n_iter = 10)
randomized.fit(X_train,y_train)
randomized.best_estimator_

#CatBoost Parameters
#iterations = np.arange(1000,1001)
#param_grid = dict(iterations = iterations)
#catb = CatBoostClassifier()
#randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 2, n_iter = 2)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

def rand_search_wrapper(refit_score='f1_score'):
  skf = StratifiedKFold(n_splits=10)
  rand_search = RandomizedSearchCV(knn, param_distributions= param_grid, scoring = 'f1', cv = skf, return_train_score= True, n_jobs = -1)
  rand_search.fit(X_train, y_train)
  y_pred = rand_search.predict(X_test)
  print('Best params for {}'.format(refit_score))
  print(rand_search.best_params_)
  print('\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))
  print(pd.DataFrame(confusion_matrix(y_test, y_pred),
               columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))
  return rand_search

rand_search_knn = rand_search_wrapper(refit_score='f1_score')

y_scores = rand_search_knn.predict_proba(X_test)[:, 1]

p, r, thresholds = precision_recall_curve(y_test, y_scores)

def adjusted_classes(y_scores, t):
    
    return [1 if y >= t else 0 for y in y_scores]

def precision_recall_threshold(t=0.5):
    
    # generate new class predictions based on the adjusted_classes
    # function above and view the resulting confusion matrix.
    y_pred_adj = adjusted_classes(y_scores, t)
    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),
                       columns=['pred_neg', 'pred_pos'], 
                       index=['neg', 'pos']))
    
    # plot the curve
    plt.figure(figsize=(8,8))
    plt.title("Precision and Recall curve ^ = current threshold")
    plt.step(r, p, color='b', alpha=0.2,
             where='post')
    plt.fill_between(r, p, step='post', alpha=0.2,
                     color='b')
    plt.ylim([0.5, 1.01]);
    plt.xlim([0.5, 1.01]);
    plt.xlabel('Recall');
    plt.ylabel('Precision');
    
    # plot the current threshold on the line
    close_default_knn = np.argmin(np.abs(thresholds - t))
    plt.plot(r[close_default_knn], p[close_default_knn], '^', c='k',
            markersize=15)

precision_recall_threshold(0.42)

lgbm = LGBMClassifier()
knn = KNeighborsClassifier()
catb = CatBoostClassifier()
xgb = XGBClassifier()
et = ExtraTreesClassifier()

#Extratrees Parameters
#n_estimators = np.arange(50,350)
#max_depth = np.arange(5,350)
#max_features = ['sqrt', 'log2']
#param_grid = dict(n_estimators = n_estimators, criterion = criterion, max_features = max_features, max_depth = max_depth)
#et = ExtraTreesClassifier()
#randomized = RandomizedSearchCV(et, param_grid, scoring = 'f1', cv = 2, n_iter = 10)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#knn Parameters
#k_range = np.arange(1,100)
#weights = ["uniform","distance"]
#algorithm = ["auto", "ball_tree","kd_tree","brute"]
#p = [1,2]
#leaf_size_range = np.arange(1,100)
#param_grid = dict(n_neighbors = k_range, weights = weights, leaf_size = leaf_size_range,p = p)
#knn = KNeighborsClassifier()
#randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 3, n_iter = 20)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#CatBoost Parameters
#iterations = np.arange(1000,1001)
#param_grid = dict(iterations = iterations)
#catb = CatBoostClassifier()
#randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 2, n_iter = 2)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#LGBM Parameters
n_estimators = np.arange(500,501)
param_grid = dict(n_estimators = n_estimators)
lgbm = LGBMClassifier()
randomized = RandomizedSearchCV(lgbm, param_grid, scoring = 'recall', cv = 2, n_iter = 2)
randomized.fit(X_train,y_train)
randomized.best_estimator_

def rand_search_wrapper(refit_score='f1_score'):
  skf = StratifiedKFold(n_splits=10)
  rand_search = RandomizedSearchCV(lgbm, param_distributions= param_grid, scoring = 'f1', cv = skf, return_train_score= True, n_jobs = -1)
  rand_search.fit(X_train, y_train)
  y_pred = rand_search.predict(X_test)
  print('Best params for {}'.format(refit_score))
  print(rand_search.best_params_)
  print('\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))
  print(pd.DataFrame(confusion_matrix(y_test, y_pred),
               columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))
  return rand_search

rand_search_lgbm = rand_search_wrapper(refit_score='f1_score')

y_scores = rand_search_lgbm.predict_proba(X_test)[:, 1]

p, r, thresholds = precision_recall_curve(y_test, y_scores)

def adjusted_classes(y_scores, t):
    
    return [1 if y >= t else 0 for y in y_scores]

def precision_recall_threshold(t=0.5):
    
    # generate new class predictions based on the adjusted_classes
    # function above and view the resulting confusion matrix.
    y_pred_adj = adjusted_classes(y_scores, t)
    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),
                       columns=['pred_neg', 'pred_pos'], 
                       index=['neg', 'pos']))
    
    # plot the curve
    plt.figure(figsize=(8,8))
    plt.title("Precision and Recall curve ^ = current threshold")
    plt.step(r, p, color='b', alpha=0.2,
             where='post')
    plt.fill_between(r, p, step='post', alpha=0.2,
                     color='b')
    plt.ylim([0.5, 1.01]);
    plt.xlim([0.5, 1.01]);
    plt.xlabel('Recall');
    plt.ylabel('Precision');
    
    # plot the current threshold on the line
    close_default_lgbm = np.argmin(np.abs(thresholds - t))
    plt.plot(r[close_default_lgbm], p[close_default_lgbm], '^', c='k',
            markersize=15)

precision_recall_threshold(0.21)

lgbm = LGBMClassifier()
knn = KNeighborsClassifier()
catb = CatBoostClassifier()
xgb = XGBClassifier()
et = ExtraTreesClassifier()

#Extratrees Parameters
#n_estimators = np.arange(50,350)
#max_depth = np.arange(5,350)
#max_features = ['sqrt', 'log2']
#param_grid = dict(n_estimators = n_estimators, criterion = criterion, max_features = max_features, max_depth = max_depth)
#et = ExtraTreesClassifier()
#randomized = RandomizedSearchCV(et, param_grid, scoring = 'f1', cv = 2, n_iter = 10)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#knn Parameters
#k_range = np.arange(1,100)
#weights = ["uniform","distance"]
#algorithm = ["auto", "ball_tree","kd_tree","brute"]
#p = [1,2]
#leaf_size_range = np.arange(1,100)
#param_grid = dict(n_neighbors = k_range, weights = weights, leaf_size = leaf_size_range,p = p)
#knn = KNeighborsClassifier()
#randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 3, n_iter = 20)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#CatBoost Parameters
#iterations = np.arange(1000,1001)
#param_grid = dict(iterations = iterations)
#catb = CatBoostClassifier()
#randomized = RandomizedSearchCV(knn, param_grid, scoring = 'recall', cv = 2, n_iter = 2)
#randomized.fit(X_train,y_train)
#randomized.best_estimator_

#XGB Parameters
n_estimators = np.arange(500,501)
param_grid = dict(n_estimators = n_estimators)
xgb = XGBClassifier()
randomized = RandomizedSearchCV(xgb, param_grid, scoring = 'recall', cv = 2, n_iter = 2)
randomized.fit(X_train,y_train)
randomized.best_estimator_

def rand_search_wrapper(refit_score='f1_score'):
  skf = StratifiedKFold(n_splits=10)
  rand_search = RandomizedSearchCV(xgb, param_distributions= param_grid, scoring = 'f1', cv = skf, return_train_score= True, n_jobs = -1)
  rand_search.fit(X_train, y_train)
  y_pred = rand_search.predict(X_test)
  print('Best params for {}'.format(refit_score))
  print(rand_search.best_params_)
  print('\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))
  print(pd.DataFrame(confusion_matrix(y_test, y_pred),
               columns=['pred_neg', 'pred_pos'], index=['neg', 'pos']))
  return rand_search

rand_search_xgb = rand_search_wrapper(refit_score='f1_score')

y_scores = rand_search_xgb.predict_proba(X_test)[:, 1]

p, r, thresholds = precision_recall_curve(y_test, y_scores)

def adjusted_classes(y_scores, t):
    
    return [1 if y >= t else 0 for y in y_scores]

def precision_recall_threshold(t=0.5):
    
    # generate new class predictions based on the adjusted_classes
    # function above and view the resulting confusion matrix.
    y_pred_adj = adjusted_classes(y_scores, t)
    print(pd.DataFrame(confusion_matrix(y_test, y_pred_adj),
                       columns=['pred_neg', 'pred_pos'], 
                       index=['neg', 'pos']))
    
    # plot the curve
    plt.figure(figsize=(8,8))
    plt.title("Precision and Recall curve ^ = current threshold")
    plt.step(r, p, color='b', alpha=0.2,
             where='post')
    plt.fill_between(r, p, step='post', alpha=0.2,
                     color='b')
    plt.ylim([0.5, 1.01]);
    plt.xlim([0.5, 1.01]);
    plt.xlabel('Recall');
    plt.ylabel('Precision');
    
    # plot the current threshold on the line
    close_default_xgb = np.argmin(np.abs(thresholds - t))
    plt.plot(r[close_default_xgb], p[close_default_xgb], '^', c='k',
            markersize=15)

precision_recall_threshold(0.28)

#initializing classifiers
classifier1 = ExtraTreesClassifier(n_estimators = 325, min_samples_split=7, min_samples_leaf=4,max_features = 0.745, max_depth = 110,criterion = 'entropy')
classifier2 = XGBClassifier()
classifier3 = CatBoostClassifier()
classifier4 = LGBMClassifier(boosting_type='dart', n_estimators=417, num_leaves=28)
classifier5 = KNeighborsClassifier(leaf_size=162, n_neighbors=3,p = 1, weights='distance')

#Stacking
sclf = StackingCVClassifier(classifiers = [classifier1, classifier2, classifier3, classifier4, classifier5],
                            shuffle = False,
                            use_probas = True,
                            cv = 2,
                            meta_classifier = CatBoostClassifier())

#List to store classifiers
classifiers = {"ET": classifier1,
               "XGB": classifier2,
               "Catb": classifier3,
               "LGBM": classifier4,
               "KNN": classifier5,
               "Stack": sclf}

# Train classifiers
for key in classifiers:
    # Get classifier
    classifier = classifiers[key]
    
    # Fit classifier
    classifier.fit(X_train, y_train)
        
    # Save fitted classifier
    classifiers[key] = classifier

# Get results
results = pd.DataFrame()
for key in classifiers:
    # Make prediction on test set
    y_pred = classifiers[key].predict(X_test)
    
    # Save results in pandas dataframe object
    results[f"{key}"] = y_pred

# Add the test set to the results object
results["Target"] = y_test 

results

from yellowbrick.classifier import ClassificationReport

classes = ["0","1"]
model = sclf
visualizer = ClassificationReport(model, classes=classes, support=True)
cm = ConfusionMatrix(model, classes=classes)

cm.fit(X_train, y_train)
cm.score(X_test, y_test)

cm.show()

visualizer.fit(X_train, y_train)
visualizer.score(X_test, y_test)
visualizer.show()